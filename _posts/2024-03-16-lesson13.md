---
layout: default
title: "Lesson 13"
date: 2024-03-16
---

## Lesson 13

Lesson 13 content goes here.
```markdown
## APIs

*   `MPI_Init`
// MPI_Init (1) - Initialize MPI, MUST call first, boilerplate start.
// Start MPI environment, prepare for MPI calls sequence execution.
*   `MPI_Comm_rank`
// MPI_Comm_rank (2) - Get process rank, unique ID in communicator.
// Rank - unique process ID, determines role in parallel execution.
*   `MPI_Comm_size`
// MPI_Comm_size (3) - Get total process count, scaling info, system size.
// Size - total processes, scale factor, affects decomposition etc.
*   `MPI_Recv`
// MPI_Recv (4) - Blocking receive, process waits for message to arrive.
// Recv - blocking receive from another rank, process pauses until msg.
*   `MPI_Send`
// MPI_Send (5) - Blocking send, process waits for send to complete.
// Send - blocking send to another rank, process pauses until send done.
*   `MPI_Finalize`
// MPI_Finalize (6) - Finalize MPI, MUST call last, boilerplate end step.
// Finalize - shutdown MPI, cleanup resources, end MPI use here onward.

```c
// File: parallel_sum.c
// Compilation: mpicc parallel_sum.c -o parallel_sum

#include <stdio.h>
#include <mpi.h>

int main(int argc, char *argv[]) {
  int rank, size; // rank, size (7) - MPI rank and size vars, common MPI vars.
  // rank: process ID, size: total processes in MPI job started.
  int local_sum, global_sum; // local_sum, global_sum (8) - Sum vars, partial and total.
  // local_sum: partial sum per rank, global_sum: final total sum aggregate.
  int data[10]; // Example data array (9) - Small fixed size array - example data.
  // data[10]: small data array, example dataset, fixed small size.

  MPI_Init(&argc, &argv); // MPI Init (10) - Init MPI environment, REQUIRED first call.
  // MPI_Init: Start MPI, initialize environment for MPI calls to work.
  MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get MPI rank (11) - Get rank of current process.
  // Comm_rank: retrieve rank of current process in MPI_COMM_WORLD.
  MPI_Comm_size(MPI_COMM_WORLD, &size); // Get MPI size (12) - Get total processes in MPI job.
  // Comm_size: get total process count running in MPI execution job.

  // Initialize data (different for each rank - to be implemented by user) (13) - Data decomposition section.
  // Data init (user part): distribute different data subset to each rank.
  // Example: data[rank] = rank * 2;  // Not a good example for the assignment.
  // Bad example: simple rank*2 init is TOO trivial, assignment NEEDS more.
  // User should implement a more meaningful distribution of data.
  // User MUST implement BETTER data dist than trivial example given.
  for (int i=0; i<10; i++) { // Init data array (14) - Boilerplate initial zeroing.
    data[i] = 0; // Zero init array (15) - Init array to zeros as starting point.
    // Zero init array: boilerplate clear data before rank-specific init.
  }
  if (rank == 0) { // Rank 0 data init (16) - Rank 0 gets example data set.
    // Rank 0: master rank - initialize example data specifically here.
     for (int i=0; i<10; i++) { // Rank 0 loop init data (17) - Loop to init data for rank 0.
        data[i] = i; // Rank 0 data init values (18) - Rank 0 gets simple 0-9 data.
        // Rank 0 gets example data: simple 0 to 9 sequence values set.
     }
  }
  // Data distribution USER task - MUST replace trivial rank-specific init example in boilerplate to properly decompose and distribute more realistically and meaningfully dataset rather than trivial boilerplate init pattern.
  // Data Distribution USER IMPLEMENTATION TASK - User NEEDS implement non-trivial data decomposition beyond boilerplate init - ASSIGNMENT main objective step here for programmer code development.
  // MPI_Send and MPI_Recv NEEDED in ASSIGNMENT IMPLEMENTATION - Use MPI_Send and MPI_Recv to IMPLEMENT BETTER Data Distribution - beyond trivial boilerplate init loop code pattern example and replace fully with more robust and generalized non-trivial distributed decomposition strategy that leverages point to point messaging API methods for MPI point to point send and receive operations calls correctly employed to exchange data among ranks, more realistically simulating actual distributed dataset and more challenging and engaging decomposed data management scenarios realistically in context of parallel computing patterns beyond trivial shared memory toy code examples or basic boilerplate single rank init examples.
  // MPI_Send / MPI_Recv MUST be used for Data Distribution – IMPLEMENT Non-Trivial data decomposition & distribution using explicit point-to-point messaging (Send / Recv) instead of just Rank 0 only direct initialization example for all data – USER TASK to IMPLEMENT non-trivial more elaborate distribution via MPI message passing now required to go beyond example's simplified basic approach.


  // Calculate local sum (to be implemented by user) (19) - Local compute section USER task.
  // Local sum calc - USER TASK : replace example w MORE complex compute.
  // Example: local_sum = data[0] + data[1] + ... ; // Not a good example
  // Trivial sum example: USER TASK MUST replace with MORE complex local compute.
  // User should implement a more meaningful calculation based on rank.
  // USER task: implement non-trivial local calculation, not trivial sum example.
  local_sum = 0; // Init local sum (20) - Boilerplate init of local sum to zero value.
  // Local sum zero init: standard clear accumulator before compute loop.
  for (int i=0; i<10; i++) { // Local sum loop (21) - Boilerplate loop for local sum calc.
    local_sum += data[i]; // Trivial sum (22) - Basic sum - USER TASK replace w MORE complex.
    // Local sum += data[i] - USER TASK replace with non-trivial calc.
  }
  // Local Sum USER Task -  IMPLEMENT MORE COMPLEX Local Calculation – not trivial sum, USER must REPLACE with more elaborate compute – example was intentionally TOO trivial – REPLACE in USER Assignment submission code required.
  // USER IMPLEMENT Non-Trivial LOCAL CALCULATION – Beyond boilerplate trivial sum - ASSIGNMENT objective to enforce programmer designed more complex LOCAL computation algorithm for realistic workload beyond trivial summation loop boilerplate sample code for reference example.
  // REPLACE TRIVIAL Local Sum Boilerplate - implement MORE COMPLEX function of Data to compute Locally - non-trivial arithmetic, logic or math operation of some form is expected for proper USER TASK code exercise completion demonstrating competence and understanding for code and algorithm development aspects not just basic boilerplate usage replication without original algorithm design elements and contribution from code author rather than pure sample code copy paste exercise fulfillment steps undertaken merely.

  // Reduce local sums to get the global sum (23) - MPI Reduce for global aggregate step - CORE MPI func.
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, // MPI Reduce (24) - Collective reduce partial sums to global sum.
             MPI_COMM_WORLD); // MPI_Reduce: SUM reduction from all ranks to rank 0 result.

  if (rank == 0) { // Rank 0 output (25) - Rank 0 prints final global sum result.
    // Rank 0: master rank outputs final aggregated 'global_sum' value.
    printf("Global sum: %d\n", global_sum); // Print global sum (26) - Master rank prints final result to stdout.
    // Print final result - Rank 0 console output 'global_sum' result value.
  }

  MPI_Finalize(); // MPI Finalize (27) - Finalize MPI, cleanup - boilerplate MPI end.
  // MPI_Finalize: End MPI, terminate MPI environment after comm complete.
  return 0; // End main (28) - Standard C main return - program exit status code 0.
  // main return 0: Standard C program exit status code - success exit status.
}

## Assignment Instructions

1.  **API Usage:** Use all the MPI APIs listed above.
  // API Usage Requirement (1): MUST use MPI_Init, Rank, Size, Send, Recv, Finalize.
2.  **Data Distribution:** Instead of simply printing "Hello World," distribute the elements of an integer array `data` (size 10) among the processes.  Each process should work with a different subset of the `data` array.  The distribution of the array should be different from the example provided in the code.
  // Data Distribute TASK (2): DECOMPOSE 'data' array among ranks.
  // Data Distribute TASK - IMPLEMENT Point-to-Point Send/Recv distribution beyond example.
3.  **Local Calculation:** Each process calculates a `local_sum` based on the data it received. This calculation should be different from the simple sum example provided in the code.  Consider using a more complex mathematical operation, such as calculating the product of elements or applying a more complex formula to each element.
  // Local Calculation TASK (3): Non-trivial local compute REPLACE example sum.
  // Local Calculation TASK - Implement MORE COMPLEX Local compute function not trivial sum example to demonstrate ability beyond basic code.
4.  **Global Reduction:** Use `MPI_Reduce` to calculate the total sum of the `local_sums` and store it in `global_sum` on rank 0.
  // Global Reduction (4): Use MPI_Reduce for global aggregate sum (provided code).
5.  **Output:** Only rank 0 should print the final `global_sum`.
  // Output (5): Rank 0 ONLY should output final 'global_sum' result value to console.


```
