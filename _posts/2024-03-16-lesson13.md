---
layout: default
title: "Lesson 13: MPI APIs and Parallel Sum Example"
date: 2024-03-16
---

## Lesson 13

Lesson 13 content goes here.

### APIs

* **`MPI_Init`**  
  - MPI_Init (1) - Initialize MPI; MUST call this first as boilerplate start.  
  - Starts the MPI environment and prepares for subsequent MPI calls.

* **`MPI_Comm_rank`**  
  - MPI_Comm_rank (2) - Get the process rank, which is the unique ID in the communicator.  
  - The rank determines the role of the process in parallel execution.

* **`MPI_Comm_size`**  
  - MPI_Comm_size (3) - Get the total number of processes (system size/scaling information).  
  - This value affects how data is decomposed and distributed.

* **`MPI_Recv`**  
  - MPI_Recv (4) - Blocking receive; the process waits for a message to arrive.  
  - The call blocks until a message is received from another rank.

* **`MPI_Send`**  
  - MPI_Send (5) - Blocking send; the process waits until the send operation is complete.  
  - The call blocks until the message has been sent to the target process.

* **`MPI_Finalize`**  
  - MPI_Finalize (6) - Finalize MPI; MUST call this last as a boilerplate end step.  
  - Shuts down the MPI environment, cleans up resources, and ends MPI usage.

---

### Example Code: parallel_sum.c

```c
// File: parallel_sum.c
// Compilation: mpicc parallel_sum.c -o parallel_sum

#include <stdio.h>
#include <mpi.h>

int main(int argc, char *argv[]) {
  int rank, size; // rank, size (7) - MPI rank and total process count.
  // 'rank' is the process ID; 'size' is the total number of processes in the MPI job.
  int local_sum, global_sum; // local_sum, global_sum (8) - Partial and global sum variables.
  // local_sum: each process's partial sum.
  // global_sum: the aggregated sum across all processes.
  int data[10]; // Example data array (9) - A small, fixed-size array for demonstration.
  // data[10]: a sample dataset of fixed size.

  MPI_Init(&argc, &argv); // MPI_Init (10) - Initialize the MPI environment. REQUIRED first call.
  // Starts MPI and prepares for subsequent MPI calls.
  MPI_Comm_rank(MPI_COMM_WORLD, &rank); // MPI_Comm_rank (11) - Get the current process's rank.
  // Retrieves the unique rank (ID) for this process within MPI_COMM_WORLD.
  MPI_Comm_size(MPI_COMM_WORLD, &size); // MPI_Comm_size (12) - Get the total number of processes.
  // Retrieves the total number of processes participating in the MPI job.

  // Initialize data (this section is to be implemented by the user for proper data distribution).
  // Data init (user part): Distribute a different subset of the 'data' array to each rank.
  // Example (trivial): Each element is initialized to 0.
  for (int i = 0; i < 10; i++) { // (14) - Boilerplate initialization: zeroing out the data array.
    data[i] = 0; // (15) - Zero initialization of each element.
  }
  if (rank == 0) { // (16) - Rank 0 initializes example data.
    // Rank 0 (master) sets up an example data set.
    for (int i = 0; i < 10; i++) { // (17) - Loop to initialize data for rank 0.
      data[i] = i; // (18) - Simple example: data set to 0, 1, 2, ..., 9.
    }
  }
  // Data Distribution USER TASK:
  // Replace the trivial rank-specific initialization with a non-trivial data decomposition strategy.
  // Use MPI_Send and MPI_Recv to distribute parts of the data array among the processes,
  // rather than having only rank 0 perform a simple initialization.

  // Calculate local sum (this section is also to be refined by the user).
  // Local Calculation USER TASK:
  // Replace the simple sum with a more complex mathematical operation (e.g., product or another formula).
  local_sum = 0; // (20) - Initialize local sum to zero.
  for (int i = 0; i < 10; i++) { // (21) - Loop over the data array.
    local_sum += data[i]; // (22) - Trivial summation; USER TASK: implement a more complex calculation.
  }
  // USER TASK: Implement a non-trivial local calculation that goes beyond a basic sum.

  // Reduce local sums to get the global sum (23) - Use MPI_Reduce to aggregate partial sums.
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, // (24) - Collective reduce operation.
             MPI_COMM_WORLD);
  // MPI_Reduce aggregates the 'local_sum' from all processes into 'global_sum' at rank 0.

  if (rank == 0) { // (25) - Only rank 0 outputs the final global sum.
    // Rank 0 prints the final aggregated result.
    printf("Global sum: %d\n", global_sum); // (26) - Output the final global sum.
  }

  MPI_Finalize(); // (27) - Finalize MPI, clean up the MPI environment.
  // MPI_Finalize: Terminates MPI; no MPI calls should be made after this.
  return 0; // (28) - End main; standard C exit code 0 indicates success.
}
```

---

### Assignment Instructions

1. **API Usage:**  
   Use all the MPI APIs listed above.  
   *Requirement:* You must use `MPI_Init`, `MPI_Comm_rank`, `MPI_Comm_size`, `MPI_Recv`, `MPI_Send`, and `MPI_Finalize`.

2. **Data Distribution:**  
   Instead of simply printing "Hello World," distribute the elements of an integer array `data` (size 10) among the processes.  
   Each process should work with a different subset of the `data` array.  
   The distribution should be implemented using MPI point-to-point messaging (i.e., using `MPI_Send` and `MPI_Recv`) and should differ from the trivial example provided.

3. **Local Calculation:**  
   Each process should calculate a `local_sum` based on the data it receives.  
   This calculation must be more complex than the simple summation shown in the example.  
   Consider applying a more advanced mathematical operation, such as computing the product of elements or applying a complex formula to each element.

4. **Global Reduction:**  
   Use `MPI_Reduce` to aggregate the `local_sum`s from all processes into a single `global_sum` on rank 0.

5. **Output:**  
   Only rank 0 should print the final `global_sum` to the console.
```
