---
layout: default
title: "Lesson 8: Parallel Array Processing with OpenMP"
date: 2024-03-11
---

# Lesson 8: Parallel Array Processing with OpenMP

## OpenMP Functions and Clauses Overview

In this section, we cover several essential OpenMP functions and clauses used to build parallel applications:

```none
omp_get_thread_num    // Get Thread ID (1) - Retrieves the unique thread number within a team.
// Thread IDs range from 0 to num_threads-1.

omp_get_num_threads   // Get Num Threads (2) - Returns the total number of threads in the current parallel region.
// Useful to determine the team size.

omp_set_num_threads   // Set Num Threads (3) - Requests a specific number of threads for a parallel region.
// The runtime may override this request.

omp_parallel          // Parallel Pragma (4) - Begins a parallel region by forking threads.
// Creates a team of threads to execute the following block concurrently.

omp_single            // Single Pragma (5) - Specifies that only one thread should execute the enclosed block.
// An implicit barrier follows the single block.

omp_barrier           // Barrier Pragma (6) - Forces all threads to synchronize at this point.

copyprivate           // Copyprivate Clause (7) - Broadcasts a private variableâ€™s value from one thread to all threads.
// Typically used after a single construct.

reduction             // Reduction Clause (8) - Enables thread-safe aggregation operations (e.g., sum, max).
// Combines values computed in parallel.
```

## Example: Parallel Array Processing

The following C program demonstrates how to use OpenMP for parallel processing on a 2D array. It initializes a sample array, computes the sum of each row, and finds the maximum value across the array.

```c
// filename: parallel_array_processing.c
// Compilation: gcc -fopenmp parallel_array_processing.c -o parallel_array_processing

#include <stdio.h>
#include <omp.h>

int main() {
    int data[8][16];   // A small, fixed-size 2D array.
    int sums[8] = {0}; // Array to store the sum of each row.
    int max_val;       // Variable to hold the maximum value across the array.

    // Initialize the 2D array with sample data.
    // Each element is computed as (row_index + 1) * (col_index + 1).
    for (int i = 0; i < 8; i++) {
        for (int j = 0; j < 16; j++) {
            data[i][j] = (i + 1) * (j + 1);
        }
    }
    // In a real-world scenario, replace this with data loading or a more complex initialization.

    // Request 8 threads for the parallel region.
    omp_set_num_threads(8);

#pragma omp parallel shared(data, sums, max_val)
    {
        int thread_id = omp_get_thread_num();   // Retrieve the thread's unique ID.
        int num_threads = omp_get_num_threads();  // Get the total number of threads.
        int local_max = 0;                        // Thread-local variable for tracking the maximum value.

        // Determine the range of rows this thread will process.
        int start_row = thread_id * (8 / num_threads);
        int end_row = (thread_id + 1) * (8 / num_threads);
        if (thread_id == num_threads - 1) end_row = 8; // The last thread processes any remaining rows.

        // Process the assigned rows: calculate row sums and update local maximum.
        for (int i = start_row; i < end_row; i++) {
            for (int j = 0; j < 16; j++) {
                sums[i] += data[i][j]; // Accumulate the sum for the current row.
                if (data[i][j] > local_max) {
                    local_max = data[i][j]; // Update the thread-local maximum.
                }
            }
        }

        // Update the global maximum value in a thread-safe manner.
#pragma omp critical
        {
            if (local_max > max_val) {
                max_val = local_max;
            }
        }

        // Synchronize all threads before printing the results.
#pragma omp barrier

        // Only one thread (by default, thread 0) prints the results.
#pragma omp single
        {
            printf("Maximum value found: %d\n", max_val);
            for (int i = 0; i < 8; i++) {
                printf("Sum of row %d: %d\n", i, sums[i]);
            }
        }
    } // End of parallel region

    return 0; // Successful program termination.
}
```

## Detailed Explanation and Considerations

- **Array Initialization**  
  The 2D array `data` is populated using a simple formula: each element is the product of its (1-indexed) row and column numbers. In a production setting, this could be replaced with data loaded from a file or generated via a more sophisticated algorithm.

- **Thread Distribution**  
  The rows of the array are divided evenly among the threads. The last thread is assigned any remaining rows if the total number of rows is not perfectly divisible by the number of threads.

- **Local vs. Global Computation**  
  Each thread computes a local maximum (`local_max`) while processing its subset of rows. The global maximum (`max_val`) is updated inside a critical section to ensure that only one thread writes to it at a time, thus preventing race conditions.

- **Synchronization Mechanisms**  
  - **Barrier:** The `omp barrier` ensures that all threads have completed their computations before any thread proceeds to output the results.
  - **Single:** The `omp single` construct guarantees that only one thread prints the final results, avoiding duplicate output.

- **Performance Considerations**  
  - **False Sharing:** The design minimizes false sharing by having each thread work on distinct rows. However, care must be taken when multiple threads write to adjacent memory locations (e.g., in the `sums` array).
  - **Critical Section Contention:** Although the critical section ensures correctness for updating `max_val`, excessive contention could affect performance if many threads frequently attempt to update the variable.
  - **Load Balancing:** The method used for row distribution is simple and works well for small arrays. For larger datasets or uneven workloads, more sophisticated scheduling techniques might be necessary.

```
