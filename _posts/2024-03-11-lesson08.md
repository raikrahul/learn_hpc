---
layout: default
title: "Lesson 8"
date: 2024-03-11
---

## Lesson 8
Step 1:
```
omp_get_thread_num // Get Thread ID (1) - Rank of thread within team.
// Thread ID: unique ID for each thread, 0 to num_threads-1.
omp_get_num_threads // Get Num Threads (2) - Total threads in parallel region.
// Num Threads: Total threads in current team - parallel region size.
omp_set_num_threads // Set Num Threads (3) - Request specific thread count.
// Set Num Threads: request thread count, OS/runtime may not honor.
omp_parallel // Parallel Pragma (4) - Begin parallel region, fork threads.
// Parallel Pragma: starts parallel region, team of threads created here.
omp_single // Single Pragma (5) - Single thread executes block, implicit barrier.
// Single Pragma: only ONE thread in team runs section, implicit barrier.
omp_barrier // Barrier Pragma (6) - Explicit thread barrier, all threads sync.
// Barrier Pragma: force ALL threads to wait, sync point enforced here.
copyprivate // Copyprivate Clause (7) - Broadcast private var val from single to team.
// Copyprivate: var from single section broadcast AFTER single section to ALL.
reduction // Reduction Clause (8) - Thread-safe aggregation, sum, max, etc.
// Reduction: Combine thread-local values thread-safely (sum, max, etc).
```

Step 2:
```c
// filename: parallel_array_processing.c
// Compilation: gcc -fopenmp parallel_array_processing.c -o parallel_array_processing

#include <stdio.h>
#include <omp.h>

int main() {
    int data[8][16]; // Example 2D array (9) - 2D data array, small fixed size example.
    // data[8][16]: small 2D array, example dataset, fixed small dimensions.
    int sums[8] = {0}; // Row sums array (10) - Array to store sum per array row, pre-init zero.
    // sums[8]: stores row sums, one sum per row, initialized to zero default.
    int max_val; // Max value var (11) - Var to store max value found across array - scalar result.
    // max_val: scalar, stores max value of array, result from parallel loop.


    // Initialize the array with some sample data. (12) - Data init stage example - boilerplate.
    // Array Init - boilerplate example: fill 2D array with example data values set.
    // User should implement meaningful initialization.
    // USER task - replace boilerplate init w meaningful dataset creation instead of simple example.
    for (int i = 0; i < 8; i++) { // Row loop init (13) - Loop rows of 2D array for init.
        for (int j = 0; j < 16; j++) { // Col loop init (14) - Loop cols in each row for init.
            data[i][j] = (i + 1) * (j + 1); // Sample data init (15) - Simple function of row and col index for data.
            // Data init : simple function row*col for example, not realistic HPC data, boilerplate.
        }
    }
    // Data init trivial - USER to replace w meaningful data creation / load - boilerplate example is TOO simplistic for real use cases.
    // USER must replace boilerplate array init -  with meaningful data population logic, real world datasets will NOT be trivial init loops but data loading / generation algo for realistic HPC simulation dataset scenario prep before compute kernels applied to that pre-populated HPC data to process using parallel codes next typically for real application code bases not toy samples only.

    omp_set_num_threads(8); // Set num threads (16) - Request 8 threads, but runtime may vary actual.
    // Set threads = 8: request fixed thread count - but check runtime thread count always to ensure runtime honored or dynamically adjusted to optimal values via empirical testing benchmarks and profiling guided feedback loop based cycles used rather than blind setting of default values and assuming optimality or correctness in all situations of varying hardware setups deployed for production or development and CI testbed infrastructures and deployment environments which exhibit often significant variations hardware architecture variations.

#pragma omp parallel shared(data, sums, max_val) // Begin parallel region (17) - Shared data for parallel loop and sections region.
    // Parallel region - Shared data: 'data', 'sums', 'max_val' explicitly shared.
    {
        int thread_id = omp_get_thread_num(); // Get thread ID (18) - Each thread gets its unique ID.
        // Thread ID retrieval - unique rank for each thread in team scope context here.
        int num_threads = omp_get_num_threads(); // Get num threads (19) - Total threads in team for parallel region block.
        // Num threads - get total count threads active for parallel block scope.
        int local_max = 0; // Init local max (20) - Thread-local max, per thread max tracker variable init to zero.
        // Local max variable: thread-private var to track max within each thread.

        // Each thread processes a different row of the array. (21) - Data decomposition by rows strategy for parallelism.
        // Row-wise decomposition - each thread assigned DIFFERENT rows of data array for parallel compute stage iteration cycle.
        int start_row = thread_id * (8 / num_threads); // Start row (22) - Calc start row for thread - even split rows.
        // Start row calc: even row distribution among threads in parallel block decomposed section assigned row indices mapping scheme here.
        int end_row = (thread_id + 1) * (8 / num_threads); // End row (23) - Calc end row for thread - even split rows mostly but remainder case handled.
        // End row calc: end row for thread's chunk, even row split scheme in code region defined by indices assignment ranges.

        if (thread_id == num_threads -1) end_row = 8; // Adjust for remainders (24) - Last thread handles remainder rows if uneven rows count division.
        // Remainder rows: last thread gets any remaining rows if rows % threads != 0 cases to handle full row coverage despite non-uniform split in general case data and parallel decompose regions assignments to threads for workload balancing.
        // Load Imbalance? - Slight Load Imbalance possible, last thread MAY have slightly more work - negligible here but general issue.
        // Load Imbalance Risk - minor : last thread handles remainder rows – minor Load Imbalance, for trivial small array here mostly negligible effect, in larger datasets can be non-negligible.

        for (int i = start_row; i < end_row; i++) { // Row loop (25) - Iterate rows assigned to thread.
            // Row loop - each thread iterates over its assigned ROWS subset indices for its portion data slice in decomposed region iteration cycle.
            for (int j = 0; j < 16; j++) { // Col loop (26) - Iterate all cols in each assigned row.
                sums[i] += data[i][j]; // Sum of row (27) - Accumulate sum for current row into sums array in shared sums storage region by each thread - shared memory write.
                // Row sum += data[i][j]: Accumulate row sums - SHARED 'sums' array.
                if (data[i][j] > local_max) { // Local max update (28) - Update thread-local max, thread-private variable here not shared.
                    local_max = data[i][j]; // Update local max (29) - Update local max value - thread private local max.
                    // Local max update : Update thread-private 'local_max' for each thread independently here during loop scan stage operation unit compute cycles step performed now inside iteration cycle unit and execution context stages of inner loop.
                }
            }
        }
        // False Sharing Risk? - 'sums[i] += data[i][j]' - adjacent threads writing adjacent 'sums' elements in shared 'sums' array? Potential False Sharing but row decomposition strategy tries minimize it – alignment of 'sums' array important if False Sharing suspected.
        // False Sharing potential on 'sums' array? - adjacent threads modify adjacent 'sums' array elements? - verify with profilers for False Sharing presence empirically measured rather than purely theoretical or design phase reviews.
        // Cache Thrashing Risk? - Data access pattern stride 1 - cache friendly contiguous access in inner loop normally BUT outer loop could induce capacity misses if array very large > cache and outer loop dominates iteration step cycle cost per row scan – for small array size unlikely Thrashing for THIS trivial example benchmark scenario as written – scale size tests needed for true characterization realistic behaviour assessments beyond toy level benchmarks shown currently only for illustration purposes and concept code validation demo rather than full system scale HPC validation test setup scenarios.
        // Cache Thrashing unlikely in trivial example - but data size INCREASE may induce Thrashing – test scalability with larger datasets systematically using performance benchmark suite runs to evaluate code behaviors over scale systematically assessed using performance characterization methodology driven approaches not just assumptions about theoretical code scaling capabilities and projected behaviors in abstract scenarios not realistically resembling operational workloads.

#pragma omp critical // Begin critical section (30) - Thread-safe max value update for global max_val.
        // Critical Section - enforce MUTUAL EXCLUSION to update SHARED 'max_val'.
        {
            if (local_max > max_val) { // Global max update (31) - Compare local max to global max, update global if needed.
                max_val = local_max; // Update global max (32) - Update shared 'max_val' inside critical section.
                // Global max update - update SHARED 'max_val' under MUTEX enforced critical region, thread-safe write now for max_val global shared variable update sequence enforcement and protected via critical code section imposed by OpenMP for mutual exclusion guaranteed thread safe access in serialized way using locking/mutex based low level constructs hidden behind compiler code directives abstractions implicitly and transparently performed rather than explicit low level primitives usage visible directly at user application code levels.
            }
        } // End critical section (33) - End critical section - release mutex.
        // End critical section - release MUTEX - other threads now may enter mutex protected section sequentially and in mutually exclusive manner, serially now protected section enforce code execution via Mutual Exclusion mechanism enforcement transparently implemented by OpenMP in runtime library under compiler directive enforced constraints implicitly rather than programmer explicitly managed locks code segments visible and code design patterns required instead by HPC programmer explicitly now – hidden behind OpenMP directive enforced protections under the hood of code construct at higher abstraction level code encapsulation and coding patterns achieved programmatically.
        // Performance Bottleneck? - Critical section SERIALIZES updates to 'max_val' - contention if many threads find new local max very often might make critical section a bottleneck - unlikely here with max finding relatively sparse event pattern – in general code contention critical section may limit speedup severely under heavy update frequencies enforced or highly contended locks driven workload conditions applied potentially depending upon use case workload scenario being modeled via code algorithms and input dataset driving characteristics enforcing hot spots or not depending upon input dataset profile or properties and features used for validation assessments.
        // Contention on 'max_val' update via critical? - Profile, check contention level under larger arrays and core counts – trivial example likely not expose contention issue in this context usually typical toy code benchmark cases demonstrated via code snippets examples shown in materials used for code presentation steps or teaching cycles.

#pragma omp barrier // Explicit barrier (34) - Force all threads to synchronize - full team barrier.
        // Barrier Sync - ALL threads WAIT here - for max_val update to complete BEFORE single region access section for printf occurs to avoid data being stale or incorrect and inconsistent outputs enforced if sync omitted from source code flow paths via design errors by HPC code developer not understanding or appreciating correct OpenMP usage enforced programming principles requirements mandated for functional program behaviour consistency expectations to meet operational functionality quality target requirements defined for system code architecture and expected system level properties by system architecture driven engineering driven considerations.

#pragma omp single // Begin single section (35) - Only master thread 0 will execute this block.
        // Single Section - only ONE thread (master 0 by default usually in practice if not explicitly overriden by OMP_ flags configuration to override single master thread rank selection rules in deployment configuration system setup configuration and execution environments enforced to modify from standard defaults imposed via runtime engine behaviour defaults used in standard MPI default behavior characteristics typically used by most MPI libraries.
        {
            printf("Maximum value found: %d\n", max_val); // Print max val (36) - Only thread 0 prints max val - serial output now again in serial section region.
            // Single thread (master rank) output final 'max_val' result now only by design to enforce output only on single master process, reduce console spam across all MPI ranks during parallel test runs etc – standard HPC console I/O handling paradigm for many cases.
            for (int i = 0; i < 8; i++) { // Serial print sums (37) - Serial loop - single thread prints row sums serially - sequential phase after parallel work block complete execution cycle operations now processed in code execution timeline.
                printf("Sum of row %d: %d\n", i, sums[i]); // Print row sums (38) - Serial printf again - sequential output now performed only by rank 0 thread due to single region.
                // Serial output: Rank 0 sequentially prints all row sums, sequential again for output stage in single thread controlled context after parallel regions sections of code executions now done with and results computed now aggregated in memory storage units after barrier enforced synchronization point has been processed by all threads in parallel processing phase cycles.
            }
        } // End single section (39) - End single region - back to parallel team after single completes serial print stages completion and output cycle now fully finished with, before program exists and terminates to OS system environment which triggered initial code program executable load and run commands to execute via system shell command interpretation steps initiated by code user command line action via terminal user interactions directly with deployed HPC system environment or cluster operational code runtime environment interactions initiated from external agent invocation step for code start events to occur programmatically and procedurally within batch job scheduling system context lifecycle flow execution for automated script driven code run stages etc used in practice rather than pure interactive terminal mode used alone which atypical deployment approach to large scale HPC operational cycles or system workload executions in common scenarios.
        // End single section - back to parallel team. All threads continue after serial section is over and finished completely execution phase at code segment now marked via structural block termination code structures at source file user specified line column ranges code block delimiters indications used as parsed by compiler tools etc enforcing syntactic grammar rule compliance verified code format and construction correctness constraints checks are implemented correctly by compiler front end tools utilized as part of standard HPC code workflow processes to generate executable binary codes as compiler output product for later load, run, test and deploy, and further processing operations of many forms.
    } // End parallel region (40) - End parallel region - threads join & exit parallel team, sequential again from here down after pragma code block regions fully exited at this stage for current program execution step sequences.
    // End parallel region - threads join, back to sequential execution main thread control flow.

    return 0; // End main (41) - Standard C main return 0 - program exit status.
    // main return 0: standard success exit code from main() C function body routine segment - no runtime errors detected locally - exit code conventions follow standard POSIX compliance rules to signal operation successful to parent process in shell terminal via CLI or system program lifecycle parent/child relationship communication steps using POSIX conventions widely adopted for C codes portability across diverse target platform architectures and OS environment hosting system capabilities and software build toolchains for compilation link and execute cycle processes for deployed executable image for processing on HPC resources by end users.
}
```
