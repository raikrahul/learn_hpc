---
layout: default
title: "Lesson 4: Parallel Vector Operations with OpenMP"
date: 2024-03-07
---

# Lesson 4: Parallel Vector Operations with OpenMP

## Introduction

In this lesson, we explore vector operations using **OpenMP** for parallel speedup.  
The code demonstrates:
- **Parallel vector initialization**
- **Parallel dot product computation using reduction**
- **Parallel vector normalization**

These techniques are essential for high-performance computing (HPC), enabling efficient processing of large datasets.

---

## Code Explanation

The following C program showcases parallel vector operations with OpenMP:
1. **Parallel Vector Initialization:** Vectors `a` and `b` are initialized with random values.
2. **Parallel Dot Product Calculation:** The dot product of `a` and `b` is computed using an OpenMP reduction clause.
3. **Parallel Vector Normalization:** The vector `a` is normalized by calculating its magnitude and dividing each element.
4. **Output:** The program prints the vector size, the computed dot product, and the normalized vector.

{% highlight c %}
#include <stdio.h>   // Standard I/O functions
#include <stdlib.h>  // Memory allocation, atoi()
#include <omp.h>     // OpenMP library
#include <time.h>    // Random number seeding
#include <math.h>    // Mathematical functions like sqrt()

int main(int argc, char* argv[]) {
    if (argc != 2) {
        fprintf(stderr, "Usage: %s <vector_size>\n", argv[0]);
        return 1;
    }

    int N = atoi(argv[1]);
    if (N <= 0) {
        fprintf(stderr, "Vector size must be positive.\n");
        return 1;
    }

    double* a = (double*)malloc(N * sizeof(double));
    double* b = (double*)malloc(N * sizeof(double));
    double* normalized_a = (double*)malloc(N * sizeof(double));

    if (a == NULL || b == NULL || normalized_a == NULL) {
        fprintf(stderr, "Memory allocation failed.\n");
        return 1;
    }

    int i;

    // 1. Parallel Vector Initialization
    #pragma omp parallel for num_threads(4) shared(N, a, b) private(i)
    for (i = 0; i < N; ++i) {
        a[i] = (double)rand() / RAND_MAX;
        b[i] = (double)rand() / RAND_MAX;
    }

    // 2. Parallel Dot Product Calculation
    double dot_product = 0.0;
    #pragma omp parallel for num_threads(4) private(i) reduction(+:dot_product)
    for (i = 0; i < N; ++i) {
        dot_product += a[i] * b[i];
    }

    // 3. Parallel Vector Normalization
    double magnitude_sq = 0.0;
    #pragma omp parallel for num_threads(4) private(i) reduction(+:magnitude_sq)
    for (i = 0; i < N; ++i) {
        magnitude_sq += a[i] * a[i];
    }

    double magnitude = sqrt(magnitude_sq);

    #pragma omp parallel for num_threads(4) shared(N, a, normalized_a, magnitude) private(i)
    for (i = 0; i < N; ++i) {
        normalized_a[i] = a[i] / magnitude;
    }

    // 4. Output
    printf("Vector size N = %d\n", N);
    printf("Dot Product: %lf\n", dot_product);
    printf("Normalized Vector a:\n");
    
    for (i = 0; i < N; ++i) {
        printf("%lf ", normalized_a[i]);
    }
    printf("\n");

    free(a);
    free(b);
    free(normalized_a);

    return 0;
}
{% endhighlight %}

---

## Key Takeaways

- **Parallel Initialization:** `#pragma omp parallel for` speeds up vector initialization.
- **Reduction for Dot Product:** The `reduction(+:dot_product)` clause safely accumulates the sum in parallel.
- **Parallel Normalization:** The magnitude is computed in parallel before each element is normalized.
- **Memory Management:** Always check for allocation failures and free allocated memory to prevent leaks.

---

## Optimizations & Considerations

- **Thread Count:** The program explicitly uses 4 threads. Adjust `num_threads` based on available CPU cores.
- **Random Number Generation:** The `rand()` function has limitations in parallel execution. Consider using a thread-safe RNG.
- **Load Balancing:** The loops distribute work evenly, but real-world applications may require additional load balancing.
- **I/O Overhead:** Printing large arrays slows performance. For benchmarking, consider reducing output.

---

## Conclusion

This lesson demonstrated how to leverage OpenMP for parallel vector operations, including initialization, dot product computation, and normalization. These techniques are fundamental in HPC applications. 

Try modifying the code to explore performance improvements and optimizations based on your system architecture.
