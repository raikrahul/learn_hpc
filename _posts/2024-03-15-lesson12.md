## APIs

*   `MPI_Init(&argc, &argv)`
// MPI Init (1) - MPI setup, MUST be first MPI call always.
// Init MPI, essential, no MPI calls before this API entry point.
*   `MPI_Comm_rank(MPI_COMM_WORLD, &rank)`
// MPI Rank (2) - Get rank of process - 0 to size-1 for rank ID.
// Rank is unique process ID in MPI_COMM_WORLD communicator set.
*   `MPI_Comm_size(MPI_COMM_WORLD, &size)`
// MPI Size (3) - Get total process count in MPI_COMM_WORLD group.
// Size is total processes, scaling factor, perf and overhead vary.
*   `MPI_Finalize()`
// MPI Finalize (4) - MPI shutdown, MUST be last MPI call in program.
// Finalize MPI - cleans up MPI resources, no MPI calls after this.

## Assignment

**Instructions:**

1.  Write an MPI program that calculates the sum of integers from 1 to *n* distributed across multiple processes.
2.  The master process (rank 0) should read the value of *n* from the command line.
3.  Distribute the integers among the processes such that each process approximately gets an equal share.
4.  Each process calculates the sum of its assigned integers.
5.  Use `MPI_Reduce` to sum up the partial sums from all processes and store the final result in the master process.
6.  The master process prints the final sum.

**Boilerplate Code:**

```c
// filename: distributed_sum.c
// Compilation: mpicc distributed_sum.c -o distributed_sum

#include <stdio.h>
#include <mpi.h>
#include <stdlib.h>

int main(int argc, char *argv[]) {
  int rank, size, n, local_start, local_end, local_sum, global_sum;
  // rank, size (5) - MPI process ID and total processes count.
  // n (6) - Total sum limit from 1 to n - problem size input.
  // local_start,end,sum (7) - Vars for local range and partial sum.
  // global_sum (8) - Final aggregated sum across ALL ranks - result.

  MPI_Init(&argc, &argv); // MPI Init (9) - Initialize MPI environment - REQUIRED.
  // MPI_Init: setup MPI, init communication world, start MPI.
  MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get MPI rank (10) - Rank ID for current process.
  // Comm_rank: get unique rank (0 to size-1), determines process ID.
  MPI_Comm_size(MPI_COMM_WORLD, &size); // Get MPI size (11) - Total number of MPI processes.
  // Comm_size: total processes in MPI_COMM_WORLD - scaling factor.

  if (rank == 0) { // Master process logic (12) - Rank 0 designated master rank here.
  // Rank 0: master, handles input, output, orchestrates reduction.
    if (argc != 2) { // Check command line args (13) -  Missing <n> arg? Usage error.
      fprintf(stderr, "Usage: %s <n>\n", argv[0]); // Usage msg (14) -  Prints correct usage syntax.
      MPI_Abort(MPI_COMM_WORLD, 1); // MPI Abort (15) - Abort MPI job on error - fail fast.
      // MPI_Abort: kill ALL MPI processes if arg missing - fail fast.
    }
    n = atoi(argv[1]); // Get n from args (16) - Read integer 'n' from command line.
    // atoi: converts string arg to int. No error handling for bad input.
    // Broadcast n to all processes
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD); // MPI Bcast (17) - Send 'n' to ALL ranks.
    // MPI_Bcast: Master rank 0 sends 'n' to ALL processes incl. self.
  } else { // Worker process logic (18) - Ranks > 0 receive 'n' from master.
    // Worker ranks (non-zero ranks) get 'n' broadcast from master 0.
    // Receive n from the master process
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD); // MPI Bcast (19) - Receive 'n' (same Bcast call).
    // MPI_Bcast (recv side): Worker ranks receive 'n' from master 0.
  }

  // Calculate the range of integers for each process. (20) - Data decomposition stage.
  int chunk_size = n / size; // Chunk size (21) - Base chunk size for each rank, integer div.
  // Chunk size: base portion of work per rank. Integer division truncates.
  int remainder = n % size; // Remainder (22) - Remainder of 'n' divided by 'size' - uneven dist.
  // Remainder: extra work units, not evenly divisible among ranks.

  if (rank < remainder) { // Handle remainder ranks (23) - First 'remainder' ranks get +1 chunk size.
    // Remainder handling: first 'remainder' ranks get larger chunks.
    local_start = rank * (chunk_size + 1) + 1; // Local start (24) - Start of local range for remainder ranks.
    // Local start for remainder ranks: adjusts start for larger chunk.
    local_end = (rank + 1) * (chunk_size + 1); // Local end (25) - End of local range, incl +1 chunk size.
    // Local end for remainder ranks: end of larger chunk assignment.
  } else { // Handle non-remainder ranks (26) - Rest of ranks get base chunk size.
    // Non-remainder ranks (most ranks) get base 'chunk_size'.
    local_start = rank * chunk_size + remainder + 1; // Local start (27) - Start of range non-remainder ranks.
    // Local start non-remainder ranks: accounts for skipped remainder work units.
    local_end = (rank + 1) * chunk_size + remainder; // Local end (28) - End of local range for base chunk.
    // Local end non-remainder ranks: end of base size chunk for these ranks.
  }
  // Load Imbalance? - slight imbalance, but should be acceptable generally for summing, but uneven split, not ideal perfect balance always guaranteed across different rank numbers tested.
  // Load Imbalance : Not perfect equal size chunks ALWAYS if n%size !=0, check large uneven split scenarios too empirically with profiling for realistic operational large scale use case performance validations required rather than assume trivial toy cases always represent practical workload characteristics always well across all deployment scenarios assumed within default assumption driven approaches without data informed verification steps employed in each context use case validated carefully systematically before just assuming theoretical design criteria holds under all operational conditions deployed in practice.


  local_sum = 0; // Init local sum (29) - Each rank starts with local sum zero value.
  // Local sum init: each rank initializes partial sum accumulator to zero.
  for (int i = local_start; i <= local_end; i++) { // Local sum loop (30) - Iterate over local range, summing.
    local_sum += i; // Partial sum (31) - Accumulate sum for assigned integer range.
    // Local sum +=i : each rank computes its partial local sum.
  }

  // Use MPI_Reduce to calculate the global sum.
  // MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  // Implementation for MPI_Reduce is left for the user.

  if (rank == 0) { // Master rank reduce (32) - Rank 0 RECV and print final sum.
    // Rank 0: calls MPI_Reduce to COLLECT final global sum & prints.
      MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
      // MPI Reduce (33) - Aggregate local sums to global sum at rank 0 - CORE step.
      // MPI_Reduce (rank 0): Reduce partial 'local_sum' to 'global_sum' via SUM op.
    printf("The sum of integers from 1 to %d is %d\n", n, global_sum); // Print global sum (34) - Final output on master rank 0.
    // Print final 'global_sum' - master rank 0 shows combined result.
  } else { // Worker rank reduce (35) - Worker ranks SEND local sum to rank 0 for reduction.
    // Worker ranks: PARTICIPATE in MPI_Reduce - send partial sum ONLY.
      MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
      // MPI Reduce (36) - Worker ranks contribute local sums to reduction too.
      // MPI_Reduce (non-zero ranks): Worker ranks send 'local_sum' for reduction.
  }
  // MPI Reduce Redundancy? - MPI_Reduce is collective, all ranks *must* call, even if result ignored for workers. But technically only master 0 *needs* global_sum in this *specific* problem â€“ workers could skip storing to global sum after reduce call completes if only master 0 is intended output destination rank for result, not all ranks, just rank 0 based on specific problem and example provided description goals. However, more idiomatic typical coding style is for ALL ranks to participate FULLY in collectives operations usually without rank conditional based divergence in collective API usage patterns enforced - hence example *correct* technically per typical best practice MPI style generally rather than micro-optimizations for only rank 0 needed the reduction result here enforced which not common typical or standard good practice MPI coding guideline.


  MPI_Finalize(); // MPI Finalize (37) - Shutdown MPI, clean up resources - REQUIRED end.
  // MPI_Finalize: end MPI, release resources, cleanup MPI library.
  return 0; // End main (38) - Standard C main return 0 for success exit code.
  // main return 0: success exit status for standard C program exit norm.
}
