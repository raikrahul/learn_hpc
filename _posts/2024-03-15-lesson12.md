---
layout: default
title: "Lesson 12: Distributed Sum with MPI"
date: 2024-03-12
---

## APIs

- **`MPI_Init(&argc, &argv)`**  
  - MPI Init (1) - MPI setup, MUST be the first MPI call always.  
  - Initializes the MPI environment; no MPI calls should be made before this.

- **`MPI_Comm_rank(MPI_COMM_WORLD, &rank)`**  
  - MPI Rank (2) - Retrieves the rank (ID) of the process (from 0 to size-1).  
  - The rank is a unique process identifier within the `MPI_COMM_WORLD` communicator.

- **`MPI_Comm_size(MPI_COMM_WORLD, &size)`**  
  - MPI Size (3) - Retrieves the total number of processes in the `MPI_COMM_WORLD` group.  
  - This value serves as a scaling factor, though performance and overhead may vary.

- **`MPI_Finalize()`**  
  - MPI Finalize (4) - Shuts down the MPI environment; MUST be the last MPI call in the program.  
  - Cleans up MPI resources; no MPI calls should be made after this.

## Assignment

**Instructions:**

1. Write an MPI program that calculates the sum of integers from 1 to *n* distributed across multiple processes.
2. The master process (rank 0) should read the value of *n* from the command line.
3. Distribute the integers among the processes such that each process approximately gets an equal share.
4. Each process calculates the sum of its assigned integers.
5. Use `MPI_Reduce` to sum up the partial sums from all processes and store the final result in the master process.
6. The master process prints the final sum.

**Boilerplate Code:**

```c
// filename: distributed_sum.c
// Compilation: mpicc distributed_sum.c -o distributed_sum

#include <stdio.h>
#include <mpi.h>
#include <stdlib.h>

int main(int argc, char *argv[]) {
  int rank, size, n, local_start, local_end, local_sum, global_sum;
  // rank, size (5) - MPI process ID and total processes count.
  // n (6) - Total sum limit from 1 to n - problem size input.
  // local_start, local_end, local_sum (7) - Variables for local range and partial sum.
  // global_sum (8) - Final aggregated sum across ALL ranks - result.

  MPI_Init(&argc, &argv); // MPI Init (9) - Initialize MPI environment - REQUIRED.
  // MPI_Init: setup MPI, initialize the communication world, start MPI.
  MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get MPI rank (10) - Retrieve the rank ID for the current process.
  // MPI_Comm_rank: obtains the unique rank (0 to size-1) for this process.
  MPI_Comm_size(MPI_COMM_WORLD, &size); // Get MPI size (11) - Retrieve the total number of MPI processes.
  // MPI_Comm_size: total number of processes in MPI_COMM_WORLD, serving as a scaling factor.

  if (rank == 0) { // Master process logic (12) - Rank 0 is designated as the master.
    // Rank 0: master, handles input, output, and orchestrates reduction.
    if (argc != 2) { // Check command line arguments (13) - Missing <n> argument? Usage error.
      fprintf(stderr, "Usage: %s <n>\n", argv[0]); // Usage message (14) - Prints correct usage syntax.
      MPI_Abort(MPI_COMM_WORLD, 1); // MPI Abort (15) - Abort MPI job on error (fail fast).
      // MPI_Abort: terminates all MPI processes if the argument is missing.
    }
    n = atoi(argv[1]); // Get n from arguments (16) - Read integer 'n' from the command line.
    // atoi: converts the string argument to an integer (no error handling for bad input here).
    // Broadcast n to all processes
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD); // MPI Bcast (17) - Master sends 'n' to ALL ranks.
    // MPI_Bcast: Rank 0 broadcasts 'n' to all processes, including itself.
  } else { // Worker process logic (18) - Ranks > 0 receive 'n' from the master.
    // Worker ranks (non-zero ranks) receive 'n' broadcast from master 0.
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD); // MPI Bcast (19) - Receive 'n' (same call on all ranks).
    // MPI_Bcast (receive side): Worker processes obtain 'n' from master 0.
  }

  // Calculate the range of integers for each process. (20) - Data decomposition stage.
  int chunk_size = n / size; // Chunk size (21) - Base chunk size for each rank (integer division).
  // Chunk size: base workload per rank; integer division truncates.
  int remainder = n % size; // Remainder (22) - Extra integers after even division.
  // Remainder: extra work units that are not evenly divisible among ranks.

  if (rank < remainder) { // Handle remainder ranks (23) - The first 'remainder' ranks get an extra integer.
    // Remainder handling: these ranks get a larger chunk.
    local_start = rank * (chunk_size + 1) + 1; // Local start (24) - Start of local range for remainder ranks.
    // Adjusted local start for ranks with extra work.
    local_end = (rank + 1) * (chunk_size + 1); // Local end (25) - End of local range (includes extra integer).
    // End of local range for remainder ranks.
  } else { // Handle non-remainder ranks (26) - Remaining ranks get the base chunk size.
    // Non-remainder ranks get the standard chunk_size.
    local_start = rank * chunk_size + remainder + 1; // Local start (27) - Start of range for non-remainder ranks.
    // Adjust local start to account for the extra work units already assigned.
    local_end = (rank + 1) * chunk_size + remainder; // Local end (28) - End of local range for base chunk.
    // End of local range for non-remainder ranks.
  }
  // Note on Load Imbalance: Slight imbalance may occur if n%size != 0, but this is generally acceptable for summing.
  // For large and uneven splits, profiling is recommended to validate performance.

  local_sum = 0; // Initialize local sum (29) - Each rank starts with a partial sum of zero.
  // Each rank initializes its partial sum.
  for (int i = local_start; i <= local_end; i++) { // Local sum loop (30) - Iterate over local range and accumulate sum.
    local_sum += i; // Partial sum (31) - Add each integer in the range to the local sum.
    // Each rank computes its partial sum.
  }

  // Use MPI_Reduce to calculate the global sum.
  // MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  // The MPI_Reduce call aggregates all local sums into the global sum on the master process.

  if (rank == 0) { // Master rank reduction (32) - Rank 0 receives and prints the final sum.
    // Rank 0 calls MPI_Reduce to collect the final global sum and then prints it.
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
    // MPI_Reduce (33) - Aggregates local sums into a global sum using the MPI_SUM operation.
    printf("The sum of integers from 1 to %d is %d\n", n, global_sum); // Print global sum (34) - Final output on master (rank 0).
    // Master prints the final aggregated result.
  } else { // Worker rank reduction (35) - Worker ranks participate in the reduction.
    // Worker processes send their partial sums via MPI_Reduce.
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
    // MPI_Reduce (36) - Workers contribute their local sums to the overall reduction.
  }
  // Note on MPI_Reduce: This is a collective operation; all ranks must call it, even if only rank 0 uses the result.
  // While only rank 0 needs the final result in this example, it is standard practice for all processes to participate.

  MPI_Finalize(); // MPI Finalize (37) - Shutdown MPI and clean up resources; REQUIRED as the final MPI call.
  // MPI_Finalize: terminates MPI, releasing resources and cleaning up the MPI library.
  return 0; // End main (38) - Returns 0 to indicate successful program termination.
  // Return 0: Standard success exit code for C programs.
}
