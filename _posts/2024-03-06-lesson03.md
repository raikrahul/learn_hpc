---
layout: default
title: "Lesson 3: Parallel Summation Using OpenMP"
date: 2024-03-06
---

<h1>Lesson 3: Parallel Summation Using OpenMP</h1>

<h2>Introduction</h2>

<p>This lesson covers <strong>parallel summation</strong> using <strong>OpenMP</strong> in C. The program demonstrates:</p>
<ul>
  <li><strong>Memory allocation</strong></li>
  <li><strong>Random number initialization</strong></li>
  <li><strong>Parallel computation using OpenMP</strong></li>
  <li><strong>Handling race conditions with critical sections</strong></li>
</ul>

<hr>

<h2>Code Explanation &amp; Annotations</h2>

<pre><code class="c">
#include &lt;stdio.h&gt; // stdio.h (1) - printf = I/O bottleneck HPC! BOF risk.
#include &lt;stdlib.h&gt; // stdlib.h (2) - malloc fail check crucial, atoi vuln.
#include &lt;omp.h&gt; // omp.h (3) - OpenMP: Race Conditions, Thread Safety Alert!
#include &lt;time.h&gt; // time.h (4) - time for srand seed - low res for HPC?

// Function to compute local sum
double calculateLocalSum(int start, int end, double* data) { // localSum (5)
    if (start &gt;= end) return 0.0; //Handle empty section (6) - Edge case check ok.
    double localSum = 0.0; // Local sum init (7) - Float error accumulate?
    for (int i = start; i &lt; end; ++i) { // Local sum loop (8)
        localSum += data[i]; // Accumulate local sum (9) - Float error risk over N.
                            // L4-Edge: Float rounding error if N very large for sum.
                            // AI &amp; ML Bug: Potential AI Model Divergence from round err.
    }
    return localSum; // Return local sum (10)
}

int main(int argc, char* argv) { // Main func (11)
    if (argc &lt; 2) { // Arg check (12) - Missing N arg error case.
        fprintf(stderr, "Usage: %s &lt;N&gt;\n", argv); // Usage msg (13) - stderr I/O slow HPC
        return 1; // Exit usage error (14)
    }

    int N = atoi(argv); // N from arg (15) - atoi input validation needed!
    if (N &lt;= 0) { // Validate N&gt;0 (16) - N non-positive input error.
        fprintf(stderr, "N must be a positive integer.\n"); // N err msg (17) - stderr I/O
        return 1; // Exit N error (18)
    }

    double* data = (double*)malloc(N * sizeof(double)); // malloc data (19) - Malloc MUST be checked for fail! Mem leak risk if not handled right!.
    if (!data) { // Malloc fail check (20) - CRITICAL malloc check - prevent crash!
        fprintf(stderr, "Memory allocation failed.\n"); // Malloc err msg (21) - stderr I/O slow
        return 1; // Exit malloc fail (22)
    }

    srand(time(NULL)); // Seed RNG (23) - time(NULL) low res, repeat seeds?
                    // L4-Edge: time(NULL) seed if loops too fast - repeat seeds!
                    //      Not HPC quality RNG. For tests ok, not for real sim.
    for (int i = 0; i &lt; N; ++i) { // Init data loop (24) - O(N) init. Cache issues?
        data[i] = (double)rand() / RAND_MAX * 100.0; // Init with rand (25) - rand() bias? HPC RNG better!
                        // L2-Algo: rand() bias - ok example, HPC needs better RNG.
    }

    double globalSum = 0.0; // Global sum init (26) - Shared, potential race cond.
                        // L3-HPC: Global var 'globalSum' SHARED in parallel region -&gt; Race Cond!

    #pragma omp parallel num_threads(8) shared(data, N, globalSum) // Parallel (27) - Race cond on globalSum? Critical misuse?
    { // OpenMP parallel block (28)
        int thread_id = omp_get_thread_num(); // Thread ID (29) - thread-private ok.
        int total_threads = omp_get_num_threads(); // Total threads (30) - thread-private.
        double localSum; // Local sum var (31) - thread-private ok, no race.

        int base_block_size = N / total_threads; // Base block size (32) - Int div, Load Imbalance?
        int remainder = N % total_threads; // Remainder calc (33) - Load balance uneven N?
        int block_size = base_block_size + (thread_id &lt; remainder? 1: 0); // Block size (34) - Load balance, still uneven workload per thread data?
        int start_index = (thread_id &lt; remainder) // Start index (35) - Load balance index calcs - error prone? Off-by-one bugs?
                          ? thread_id * (base_block_size + 1)
                          : remainder * (base_block_size + 1) + (thread_id - remainder) * base_block_size;
        int end_index = start_index + block_size; // End index (36) - Potential off by one here too in index math for load balance block split!

        localSum = calculateLocalSum(start_index, end_index, data); // Local sum call (37) - Private sum calc. No Race Condition HERE, inside local sum function isolated OK.

        #pragma omp critical // Critical section (38) - Serial bottleneck! Perf impact HUGE! Contention.
        { // Critical section start (39) - Serialize globalSum update - BAD perf!
            globalSum += localSum; // Global sum += (40) - Protected but serializes! Perf KILLS.
                            // L2-Algo: Critical section SERIAL - Amdahl's law = perf limit.
                            // L3-HPC: Contention on lock. Thread Starvation. Perf tank.
                            // L4-Edge: Deadlock risk with nested criticals in real code!
                            // CTO Rec: Atomic ops BETTER, reduction PRAGMA BEST!
        } // Critical section end (41) - Serial section ends - perf hit over.

        printf("Thread %d: Local Sum: %lf (Indices: %d to %d)\n", thread_id, localSum, start_index, end_index - 1); // Thread printf (42) - Parallel I/O - serialization. Perf impact!
    } // End parallel (43) - Implicit barrier - Load Imbalance? Starvation?

    printf("Global Sum: %lf\n", globalSum); // Final global sum print (44) - Serial print, ok now.
    free(data); // Free data (45) - Memory MUST be freed - leak if missing!
                // Memory &amp; Cache Issues: Memory leak if free MISSING. Valgrind!

    return 0; // Exit main (46) - Clean exit - no HPC bugs on exit.
}
</code></pre>

<hr>

<h2>Key Takeaways</h2>
<ul>
  <li><strong>Memory Allocation Risks</strong>: Always check if <code>malloc()</code> fails.</li>
  <li><strong>Race Condition Prevention</strong>: Protect shared variables using <code>#pragma omp critical</code> (or <code>#pragma omp reduction</code> for better performance).</li>
  <li><strong>Floating-Point Precision Issues</strong>: Summing large arrays can introduce rounding errors.</li>
  <li><strong>Load Balancing Issues</strong>: Ensure all threads get an equal workload.</li>
  <li><strong>I/O Bottlenecks</strong>: <code>printf()</code> inside parallel sections slows execution.</li>
</ul>

<hr>

<h2>Optimizations for HPC Workloads</h2>
<ul>
  <li><strong>Use <code>#pragma omp reduction(+:globalSum)</code> instead of <code>#pragma omp critical</code></strong></li>
  <li><strong>Consider high-precision floating-point techniques to minimize summation errors</strong></li>
  <li><strong>Use better random number generators (e.g., MT19937) for scientific computations</strong></li>
</ul>

<hr>

<h2>Conclusion</h2>
<p>This program demonstrated <strong>parallel summation with OpenMP</strong>, highlighting <strong>race conditions, memory allocation risks, and load balancing issues</strong>. OpenMP allows efficient <strong>multi-core computation</strong>, but <strong>proper synchronization and load balancing</strong> are critical for performance.</p>
