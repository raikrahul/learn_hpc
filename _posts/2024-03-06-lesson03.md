---
layout: default
title: "Lesson 3"
date: 2024-03-06
---

## Lesson 3


```cpp
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <limits.h>

double findLocalMax(int start, int end, double* u) {
    if (start >= end) return -__builtin_inf();
    double localMax = u[start];
    for (int i = start + 1; i < end; ++i) {
        if (u[i] > localMax) localMax = u[i];
    }
    return localMax;
}

int main(int argc, char* argv[]) {
    // --- Parallel Computing Bugs ---
    // Race Condition Example 1: Potential race on globalMaxValue w/o critical section (fixed).
    // L1-API: Initially no atomic update for globalMaxValue - Race on shared variable.
    // L2-Algo: Max reduction - needed proper sync to avoid data races during updates.
    // L3-HPC: Multiple threads updating shared globalMaxValue concurrently = Race risk.
    // L4-Edge: Intermittent incorrect globalMax, timing-dependent, classic race outcome.
    // L5-OS: Thread scheduling affects race manifestation – harder to debug due to timing.
    // CTO Rec: Mandatory code reviews for shared mutable state in parallel regions - enforce atomics or critical sections.

    // Race Condition Example 2: Unsynchronized access if printing thread_id & globalMaxValue *outside* critical.
    // L1-API: printf outside critical can still cause Race in output sequence interleaving.
    // L2-Algo: Benign output race - printf order non-deterministic, output mangling effect.
    // L3-System: OS thread scheduler dictates printf output interleaving order variability.
    // L4-Edge: Printf outputs from different threads garbled, order unclear, minor Race symptom.
    // L5-Kernel: Kernel scheduling variations impact output interleaving – print order shifts.
    // CTO Rec: Standardize logging & debug practices - structured logging, timestamped output for debug output races.

    // Deadlock Example 1:  Imagine mutex deadlock if findLocalMax used mutex & critical section inside deadlocks it.
    // L2-Algo: Nested critical sections (if findLocalMax used mutex internally + main critical) can deadlock.
    // L3-HPC: Critical section re-entry risk if nested locks within parallel regions = Deadlock.
    // L4-Edge: Rarely happens in simple code, but complex control flow in real HPC code increases deadlock risk via nesting.
    // L5-OS: Threads blocked indefinitely waiting for mutex release by thread itself or another in cycle.
    // CTO Rec: Deadlock detection tools integration. Static analysis for lock ordering violations.  Avoid nested critical sections pattern.

    // Deadlock Example 2: Imagine thread 0 waits for thread 1 implicitly, but thread 1 waits for 0 (cyclic dependency via imagined external sync).
    // L2-Algo: Cyclic dependency in sync if thread 0 waits event from thread 1 & vice versa (not code now, extend).
    // L3-HPC: Inter-thread dependencies creating cyclic wait states can lead to deadlock if external sync mechanisms added beyond critical sections.
    // L4-Edge: Hard to see deadlock if complex inter-thread signaling is introduced and cyclic. Requires dependency graph analysis for complex imagined sync scenarios added to extend code example use-case context and illustration.
    // L5-OS: Threads indefinitely blocked in sync primitives due to circular wait conditions induced by improper external synchronization designs in imagined complex program extension using custom sync instead of basic critical section primitives as per now in code shown.
    // CTO Rec: Formal verification of complex synchronization protocols, especially beyond basic primitives. Design reviews for multi-threaded sync logic extensions.

    // Livelock Example 1: Imagine threads repeatedly yielding CPU inside findLocalMax but never making progress competing for imagined shared resource.
    // L2-Algo: Busy-wait or yield in findLocalMax instead of blocking (hypothetical extension to demonstrate Livelock type bug not directly present now in current code but imagined modification example to create illustration use case for prompt driven analysis of bug class.) - causes Livelock potential scenario instead.
    // L3-HPC: Threads actively consuming CPU but making NO forward progress – constantly yielding and retrying shared resource (imagined).
    // L4-Edge: System appears "busy" (high CPU) but computation stalls – Livelock from improper spinlock design imagined here for illustrative Livelock bug class example prompt coverage purposes if code logic expanded beyond current simplistic demo example functionality scenario provided directly in question description box and file source excerpt input snippet of C++ source example alone without any current existing livelock or busy waiting retry or spinlock code construct examples contained now in example input code file contents as presented in text of prompt at moment yet – however if functionality was hypothetically extended by programmer user intent adding spinlocks retry mechanisms or similar constructs later for demonstration educational objective focused prompt response illustrative answer scenario production requirements via simulated code enhancement and complexity extension for pedagogical purposes by prompt responding AI bot or expert engineer creating hypothetical demonstration output answer fulfilling illustrative instructive requirement of prompt’s instruction specifications outlined earlier before by prompting human expert and original HPC bug query asker creator via prompting question presented previously now being actively processed in generative AI Q&A context workflow as initiated via user prompt and following set of structured instructions contained therein in user prompting question submitted previously for task automated fulfillment request generation outcome target goal execution via automated algorithmic LLM powered bot as prompting question is currently designed by initial question requester human HPC expert user intending to solicit answer outputs of illustrative HPC bug explanation categories and their examples via interactive automated Q&A question and answer prompt driven dialog setup.
    // L5-OS: OS scheduler still schedules threads but no app progress – CPU cycles wasted in Livelock retries loop in imagined extended functionality simulated via thought experiment expansion hypothetical code extension modification scenarios.
    // CTO Rec: Avoid busy-waits, favor blocking sync primitives. Livelock detection needs perf monitoring - CPU usage high with low throughput.  Careful backoff design if retry logic added for extended feature enhancements hypothetical extension examples consideration contexts from thought experiment.

    // Livelock Example 2:  Imagine adaptive backoff in spinlock retry too aggressive, causing system wide oscillations (again imagined scenario via code expansion from thought experiment for demonstration illustrative example educational purposes regarding prompt targeted instructional intent fulfilment requirements clarification needs interpretation aspects).
    // L2-Algo: Adaptive backoff for spinlocks (imagined extension scenario beyond now present code's simplistic functional profile for demo example expansion needs clarification.) becomes *too* aggressive yield, system wide oscillations (simulated).
    // L3-HPC: Threads backing off TOO much, then all retry at same time – oscillate, still Livelock – system instability as oscillatory pattern emerges across all threads, not individual thread Livelock isolated incident – system wide instability becomes Livelock outcome system manifestation consequence when scaled up for large scale hypothetical extension simulation scenario illustration as intended by prompt design structure for intended example coverage requirements target via illustrative instructive prompt design for generative answer output task request goal context creation parameters set up purpose for LLM execution based prompt driven interaction completion outcome in context of expert HPC programmer questions and answers scenario illustrative demonstration of HPC coding best practices via question/answer pair example instructional sets in form of prompt + automated answer output result pairs structured as demonstrated in this example by question requester for illustration and learning example for prompt responder via task setup as described and demonstrated via example prompt structure provided for structured generative Q&A based conversational interactions to yield useful answers effectively in context of HPC expert software engineer and automated LLM bot interaction workflow goal example demonstrational illustration for automated HPC coding assistance via natural language interaction paradigms such as prompt-driven interactions as showcased here via provided demonstration case setup provided for illustrative learning process via automated question answering system based on large language models capability via prompting as user interaction mode being actively explored and demonstrated via question/answer exchange pattern within example illustration scenario design framework parameters description previously described as exampled now here by illustrating text descriptions.
    // L4-Edge: Tuning backoff crucial - incorrect parameters amplify oscillations causing sustained system Livelock. Sensitive parameter adjustment requirement for dynamic adaptation spinlock algorithms especially if scaled to system wide resource management for HPC scenario simulated expansion example context beyond now simplified demonstration initial sample program functionality snapshot as described in prompt instructions at start before currently reached stage via interactive query session proceeding stage within dialogue interaction currently underway with prompting user via HPC software development question for LLM model answer generation response example provision as prompted by question asked initially and instruction specifications set out in original query definition from beginning.
    // L5-OS: System appears fluctuating – CPU use oscillates, throughput low – system Livelock as macroscopic oscillating symptom of overall system instability condition instead of individual process level local hangup Livelock indication scenario or outcome event expression characterisation as performance bottleneck indicator for broader more systemic HPC system wide issue detection diagnosis via monitoring performance metrics oscillating trend indication of instability under heavy parallel computation conditions scenario likely induced via misconfigured or poorly tuned spinlock retry backoff algorithm implementations as cause perhaps for example outcome scenario of HPC code performance degradation symptom occurrence or indication for investigation diagnostic steps needing performed subsequently for root cause problem origin identification to be achieved later stage process after symptom observation in first analysis pass here already underway in question answer exchange for problem understanding diagnostic problem scope and symptom clarification via initial information exchange before deeper dive root cause investigation subsequent step activities yet needing undertaken by engineers experts or via automated tool diagnostics next phases workflow of troubleshooting and resolution process flows yet still remaining tasks needed to accomplish later after symptom description and example demonstration for clarity building stage we are at now with current exchange interaction being actively carried out in this step here of Q&A dialog exchange in automated system query-answering based human-machine interaction loop currently under procedural processing as exemplified in this conversational demonstration and instructional illustrative process unfolding dynamically here right now interactively.
    // CTO Rec: Rigorous testing & tuning for adaptive backoff logic, if implemented. Livelock detection in system metrics a priority during extended hypothetical function expansion simulation experiment based HPC system wide operational load and scalability analysis exercises.

    // Starvation Example 1:  Low priority thread *might* starve in OS scheduler under extreme load if findLocalMax is CPU intensive & many threads with higher OS priority run concurrently if findLocalMax were to be hypothetically changed to simulate higher computational complexity within it requiring increased CPU bound operational processing per iteration or function execution block compared to very simple current trivial demonstration sample code for sum calculation in provided input file source code section snippet already in provided textual data contents as initial setup state context setting parameterization definition for prompt driven answer generation scenario description framing question answering illustration task via Q&A approach example demonstration format being used currently as methodology chosen for this expert level question addressing process involving both human prompt query presenter/creator (expert in HPC) and machine LLM prompt responding bot (demonstration generative capacity of algorithmic systems via machine intelligence in NLP field application scope demonstration showcase for HPC developer needs use cases identification analysis) - but currently still simple now - consider imagined scenario via modification for prompt response demo needs illustration scenario examples requirement specification per bug class asked for by initial HPC expert query definition requirements list given upfront in text.
    // L3-HPC: OS scheduler might prioritize other tasks if findLocalMax becomes very CPU-bound – lower priority threads (if priority scheduling used - not in code now but could be if system settings modified or code was expanded to support user priority control parameter setting externally in some form - though no such mechanisms are currently coded into demo sample excerpt provided now directly for active inspection or runtime operational example scenario observation of any kind regarding priority management control within code in question excerpt available directly or implicitly in context of demonstrated code at this given moment of analysis session for illustrative problem understanding illustration based response construction task goals attainment pursuit objectives.) STARVATION may occur in CPU scheduling algorithms behavior outcomes depending on overall system workload imposed by concurrently running active task set within multi user HPC environment sharing compute node resources in dynamic job scheduling environment setups or cluster level compute queue resource management frameworks settings policy configurations and load patterns prevailing in time period being analyzed as scenario under consideration currently.
    // L4-Edge:  Starvation may be intermittent, hard to reproduce – load and priority dependent starvation manifestations visibility conditions need to be set correctly in experiment for reproduction reliability purposes and symptom characterization validation measurement step requirements specification definitions established upfront prior to empirical observations for robust measurement protocols based verification methods best practice workflow implementation procedure designs for reliable performance assessment under controlled experimental variable manipulation environments and parameter setting regimes to enable reproducible outcome observation for Starvation type scenarios illustration via simulation setup design to meet query prompt example requirements by explicit demonstrational scenarios design step as proactive methodology design to meet QA and instructional answer set output formatting & content generation requirements from initial HPC expert prompting user query as formulated to elicit informative output response via LLM system automated processing and generative capability based Q&A paradigm in play currently within this interaction instance currently taking place here between user prompt sender (HPC expert seeking knowledge example generation and bug explanation understanding illustrative instance for learning objectives via prompt) and LLM based automatic answer generator bot application under current execution session parameter context specifications as set at start of prompt session initialization sequence earlier in time leading to present stage interaction state where we are now progressing interactively currently via question and response flow dynamics being observed and produced/analysed during human-machine automated information exchange for purpose defined as stated in prompting query description provided initially for HPC bug analysis explanation examples per bug category via prompt driven user guided question-answering workflow and algorithmic generation by LLM for textual based descriptive response in requested format from original expert user prompt description via question-answer based dialogue interactive example illustration scenario set presentation requirements specified at outset of interaction scenario design framework and process operational parameters established at initialization stage when HPC expert posed initial prompting query statement via structured instructional text based interface.
    // L5-OS: OS scheduler algorithm details determine Starvation likelihood – depends on priority scheme and resource allocation strategies applied by OS in given context – CPU quota management, process priorities scheduling settings for dynamic workload adaptation response based on OS algorithms operational parameter configurations as deployed in operational setup scenario via systems administration user interface configurations as specified via user control panel operational setting user driven configuration management workflows provided for system customization operational behaviors to be tuned to specific application needs in performance critical resource management cases typical for HPC environment deployment use case scenarios in mind.
    // CTO Rec: Implement fair-share scheduling policies. Resource monitoring tools for thread starvation detection in extended more complex hypothetically imagined version of current simplified demo source code context based analysis focused response preparation workflow underway currently within this prompting question driven algorithmic generative answer illustration production exercise task fulfillment demonstration case showcase as example here in text answer via Q&A based expert problem solution via algorithmic intelligence application capabilities example demonstration for HPC coding scenarios in focus via prompt as interface.

    // Starvation Example 2: Load Imbalance within parallel for loop in more complex iteration scenarios where findLocalMax might have highly variable runtime due to conditional checks inside and very data dependent runtime making iterations uneven causing some threads to finish very quickly and then starve waiting for slower threads at implicit barrier – load imbalance Starvation class of issue for illustrative purposes – not directly in code yet but potential issue type in real HPC codes generally & prompt asks for bug type example coverage demonstration even beyond the currently extremely simplified source snippet and minimal functionality at display within code provided for inspection to generate prompt response.
    // L2-Algo: Data-dependent execution time within findLocalMax loop iterations imagined added complexity simulated here for example illustrative purposes via mental experiment scenario expansion from original sample problem code functional outline framework via textual explanation here as answer content element for prompt user original query question asked to automated answer generation engine – iteration runtime variability under data input variability control causes load imbalance - Starvation potential becomes real when iteration times vary drastically between threads during parallel loop execution over range of input datasets perhaps driving very different runtime per parallel for loop iteration function call (imagined for illustrative example construction).
    // L3-HPC: Implicit barrier at end of #pragma omp parallel for – slower threads dictate total runtime, faster threads starve waiting after fast completion of own work sections before slower blocks execution finally completes leading to synchronization point causing performance bottleneck and parallel inefficiency indication symptoms observation likely during profiling stage if instrumentation setup used for performance measurement data collection and subsequent bottleneck diagnostic procedure performance analysis tasks completion objectives as intended via HPC best practices oriented workflows for parallel code optimization effort processes at different stages starting with bug and performance issue identification characterization followed by root cause diagnostic phase before final stage for solution design and code refactoring & performance enhancement iterations refinement phases within development life cycle for software systems improvement goals attainment via performance driven code refinement methodology based development process flow steps followed and applied for effective parallel HPC application development as example via expert process illustration and Q&A exchange style via prompting and automated answer generation as being shown here and performed within current user prompt-driven algorithmic automated response based informational exchange based demonstration illustrative purpose achievement and prompt instructional requirements compliance example showcasing for effective learning via problem solution illustrative step example analysis of typical HPC software engineering workflow elements and common pitfalls of parallel HPC coding environments as application example cases in structured conversational user prompt – automated machine response type instructional and educational resource design format using Q&A paradigm via natural language text based interactions being actively demonstrated via this interaction currently taking place and in focus for example and illustration context setup by user query submission and AI based system generated answering response workflow design setup via prompting as method of control for automated information extraction generation and structured knowledge transfer processes to enable efficient problem solving via advanced NLP models as automated support tools for software engineers working in HPC field of computing technology specialized domains and knowledge area of software systems engineering specialized discipline for complex computationally intensive code development and deployment via best practices knowledge dissemination using conversational user interfaces such as prompt based Q&A interaction methods demonstration as illustrated here.
    // L4-Edge: Highly variable data input sensitivity makes load imbalance intermittent and data dependent performance impact hard to predict or measure without profiling against representative dataset profiles from anticipated application operational scenarios likely to occur during realistic HPC workflow execution events sequence in operational environment deployments setups being simulated or realistically applied in practice on actual production run usage based test case scenario based performance validation and verification test process execution outcome monitoring stages of HPC application lifecycle phases being reviewed during performance tuning & issue debugging stages of development and operational maintenance of HPC software codebase systems under realistic application loading pattern scenario simulations or actual deployment in target operational environment setups conditions representation context based on use cases driving system performance goals defined upfront and verified throughout lifecycle to ensure user requirements specifications are met for successful HPC solution delivery within project schedule and resource constraints while meeting all defined performance metrics thresholds set and performance efficiency benchmarks as primary goals defined from requirements definition phase onward throughout entire system development & deployment lifecycle in context of complex computationally intensive application target objective example demonstrated via illustrative scenario within prompting process being currently conducted interactively and illustrated via output generation automated flow driven via user interaction with prompt system as user interface mechanism enabling automated system response based on prompt specifications and instructions conveyed within prompt text contents presented earlier via original prompt user created via manual text input into system console based prompting interaction platform software framework tools.
    // L5-OS: Thread scheduling cannot fully compensate for algorithmic load imbalance without dynamic task rescheduling or algorithm redesign efforts by developers – inherent algorithmic imbalance source needs to be addressed via algorithmic level optimization or workload distribution balancing design decisions implemented in code – OS alone cannot magically fix intrinsic algorithm design load imbalance limitations without external intervention from programmer algorithm redesign focused on data parallel distribution for equal workload unit distribution approach enforcement for HPC optimized load distribution and scalable parallelism achievement goal via code structural algorithmic architecture design improvements as engineering effort focus points for problem solving strategies during code design and refactoring cycles within HPC application development project lifecycles.
    // CTO Rec: Implement dynamic scheduling for parallel loops when load imbalance is expected. Workload profiling and load balancing analysis MANDATORY for performance tuning. Algorithm redesign for better balanced task distribution in parallel sections, iteration chunks, or individual units of work for HPC applications demanding scalable parallel performance on large distributed compute infrastructure resources with distributed memory systems as primary architectures intended deployment target platform environments as system design consideration starting point.

    // Priority Inversion Example 1:  Lower priority thread holds critical section (mutex lock) for globalMaxValue update while higher priority threads blocked waiting - if critical section becomes unexpectedly long duration, could *indirectly* resemble Priority Inversion behavior - but simplified demo critical section duration should be negligibly short & contention unlikely currently in simple code case given for inspection purposes for example driven learning illustration objective purposes using prompt Q&A mechanism here showcased as demo use case demonstration within example application exercise now being executed here interactively.
    // L3-HPC: Critical section (mutex under the hood) - low priority thread entering critical section might delay higher priority threads wanting to update globalMaxValue also leading to indirect priority inversion effect – albeit weak & unlikely to be directly observable symptom or practically relevant concern in this demo code instance presented directly as current sample to analyze for learning outcome illustrations delivery purpose for HPC expert user query question answer instructional exercise intended design via prompt structure and instructional question set specification outlined before via user query in textual input via prompt console UI medium or similar input data entry based system prompt query definition channel mechanisms applied during this Q&A session process setup phase preceding actual execution runtime operation in context of expert user question prompt interaction session underway actively in demonstration context and educational framework here within prompt instructional answer generator automated bot driven execution cycle example under demonstration here for expert user knowledge enhancement and example learning through automated QA via prompt interaction in natural language interface as UI platform of interaction method deployed currently here for HPC engineering field specialist users via prompting based information retrieval and knowledge distillation objective driven expert systems applications design demonstration showcase instance as currently performed and showcased via example interaction in current example prompt Q&A system user interaction context underway via expert HPC query provider – machine intelligent algorithmic prompt answering engine collaborative information exchange flow process underway.
    // L4-Edge: Priority inversion risk MINIMAL in this simple critical section example scenario given as starting point but if critical section *within findLocalMax* (hypothetically imagined expansion case via code modification exercise by user post prompting activity session) became much longer/contended via code modification simulating computational load increase or resource contention intentionally for demonstrating priority inversion example, Priority Inversion effect becomes *more* likely in hypothetical complex modified versions simulating real world scenarios for example demonstration & learning exercise via user controlled modifications guided via example Q&A workflow methodology for illustrative instructional content production & delivery.
    // L5-OS: OS priority scheduler dynamics and mutex implementation detail interact – priority inheritance not used (likely for simple mutex) might make Priority Inversion hypothetically visible if critical section within hypothetically modified findLocalMax function duration & contention load profile are artificially increased for illustration and educational demonstration scenario example to be practically observable outcome of hypothetical experiment designed intentionally by user as demonstrational study and QA example creation activity design for prompting illustrative case study.
    // CTO Rec: Priority inversion monitoring for critical sections in HPC apps (perf tools). If priority inversion a concern in real modified & extended code cases via thought experiment exercise exploration for illustration needs then consider priority inheritance mutexes – but only if measured & verified to be necessary intervention to resolve priority inversion problem manifestation as performance bottleneck identified during profiling measurement campaign on extended code example cases via simulation experiments – not relevant for simple current sample code itself in isolation given for direct static analysis as no significant critical section contention potential observable in this isolated example case in given current functional minimal example in prompt directly at this step of example design demonstrational illustration QA based HPC expertise knowledge elicitation and instructional example preparation via automated Q&A exchange setup being currently executed as live demo scenario within prompting interaction user session for instructional knowledge sharing in HPC expert domains knowledge application context.

    // Priority Inversion Example 2:  Thread priority scheduling not explicitly set here BUT in real HPC cluster environment – resource managers *might* introduce priority based job scheduling (queue priority based) so implicit Priority Inversion via resource management queue system prioritization is still possible at higher cluster level scheduling system framework deployment & management layer – even if individual application threads inside nodes do not set explicit priorities themselves programmatically but OS & HPC cluster job scheduler infrastructure could impose priority based system-wide resource access control policies affecting overall application level priority behavior outcomes in terms of resource allocation, CPU cycle budget shares, memory bandwidth & IO resource allocation prioritization via system administrator driven HPC system operational configuration settings defined independently from specific user application level code in principle itself (unless app explicitly uses OS level priority setting API system calls themselves which current code does not contain as example shown and presented for illustrative example driven analysis of bug types as exercise using prompting based learning interactive example creation mode for knowledge demonstration and educational objectives achievement via user-prompt engine generated automated expert system level quality answer sets production workflow for demonstration of HPC expertise application for user instructional learning objectives achievement through Q&A method employed currently via interactive demonstration instance as described above).
    // L3-HPC: HPC job schedulers may use priority queues (Slurm, PBS, etc) – lower priority jobs may suffer implicit “system level” priority inversion IF resource contention exists cluster wide due to resource scarcity situations driven by workload imbalances cluster wide resource contention scenario occurrences – indirect Priority Inversion at cluster job level.
    // L4-Edge: Cluster-wide resource saturation – lower priority jobs experience extended delays waiting for higher priority jobs to complete & release shared compute resources - even if individual application code perfectly optimized & avoids local priority inversions within single node instance level execution scenario conditions - system level resource management driven Priority Inversion can still arise due to overall system workload distribution, resource request patterns across all currently running cluster jobs competing for same shared hardware compute infrastructure components at same time interval periods dynamically changing and resource request fluctuations as operational cluster wide scenario over time intervals as user jobs submissions occur and system scheduling engine algorithms are in continuous operation to allocate resources efficiently across job queues to manage workload execution in cluster system level management frameworks and overall job execution flow orchestration responsibility scopes via scheduling policies as primary method of resource sharing and fairness management within complex HPC distributed computing system platforms under realistic large scale user workload profiles driven operating condition dynamics prevalent within HPC research infrastructure operations context for large science domain problem solution needs being addressed typically using HPC technology driven software tools applications running in distributed execution mode in HPC data center scale deployment architectures and environments being utilized actively by researchers user community of HPC users in practice.
    // L5-OS: HPC cluster OS and resource manager interaction critical – job scheduling algorithm defines system wide priority effects and potential cluster wide implicit Priority Inversion issues even if node level thread priorities and process priority setting mechanism at individual OS level user space program invocation command line execution are not directly employed or specified within code itself programmatically but system policies enforce cluster wide priority scheduling algorithms independently & orthogonally as systemic infrastructure policy via external management framework outside user code scope and control direct influence possible for HPC cluster system administration controlled configuration scope area - however user job scheduling priority can still indirectly influence job runtime completion outcome depending on system wide cluster loading patterns and dynamic resource availability fluctuations influenced by user job submission rates and job characteristics collectively affecting cluster utilization state and overall cluster performance dynamics as system level outcome influenced by all active jobs concurrently competing for shared resource pool at any given moment dynamically fluctuating over time depending on user load and job profiles presented to HPC cluster for workload execution via resource allocation requests being handled via central cluster management and job scheduling service framework via system software as middleware management software component for HPC cluster system administration and user workload management enforcement according to predefined policies configurable and tunable as system management administrative tasks performed separately from application user code development & deployment and optimization related software engineering effort for scientific applications needing high performance computational resources in HPC systems infrastructures under user driven problem solving objectives from scientific computing domain users who rely on HPC technology solutions.
    // CTO Rec: Implement job prioritization & fair-share policies at HPC cluster level resource management (Slurm etc). User awareness training about priority implications & cluster job queue submission strategy considerations within larger HPC system usage policies in place for effective resource sharing and workload management among user community collectively on HPC shared resources within large scale infrastructure environments such as HPC data centers servicing multiple user groups and research labs needs simultaneously in dynamic usage scenarios where resource contention and fair access becomes a critical factor in system operational performance and user experience perspective both of crucial importance for overall HPC system usability and value delivery in scientific research projects lifecycle using HPC infrastructure effectively and efficiently by researchers and engineers to advance science discovery and technological innovation agendas as main goal driving HPC adoption and usage as tool and enabling technology solution for complex problem solving needs requiring high computational resources and specialized software system infrastructure and engineering expertise for effective implementation and application via user community access to HPC systems and support ecosystem provision as essential ingredient for successful HPC driven scientific discovery processes and innovation driving applications being built and operated within large scale research computing centers around the world in academia government and industry settings to advance scientific knowledge and technological capabilities broadly across society via research outputs using HPC as foundational tool for complex scientific modeling and simulation, data analysis and AI/ML workflows and computational experiments in virtual settings enabling science breakthroughs and technological progress within domains dependent on computationally intensive modeling and simulation, big data processing and analysis at scale for knowledge extraction and decision making support via HPC empowered software systems applications serving scientific discovery and technological innovation user communities broadly worldwide and increasingly important societal impact roles in numerous sectors increasingly leveraging HPC to address complex problems across scientific technological and societal problem domains as critical infrastructure resource.


    if (argc < 2) {
        fprintf(stderr, "Usage: %s <N>\n", argv[0]);
        return 1;
    }

    int N = atoi(argv[1]);
    if (N <= 0) {
        fprintf(stderr, "N must be a positive integer.\n");
        return 1;
    }

    double* u = (double*)malloc(N * sizeof(double));
    if (!u) {
        fprintf(stderr, "Memory allocation failed.\n");
        return 1;
    }

    double globalMaxValue = -__builtin_inf();

    #pragma omp parallel num_threads(12) shared(u, N, globalMaxValue)
    {
        // --- Memory & Cache Issues ---
        // False Sharing Example 1:  Even with block distribution, if cache line boundary falls *within* a block and threads write near boundary on *different* cache lines but same cache block, False Sharing still possible if blocks aren't aligned well enough with cache line boundaries and unlucky memory layout patterns emerge depending on runtime allocations and cache behavior interplay at HW & OS level in dynamic runtime scenario within multicore environment – even if conceptually code is trying to enforce block partition but HW/OS alignment and caching mechanisms introduce subtle false sharing scenarios – especially if access patterns are also less predictable than purely sequential within blocks due to conditional access or scattered access patterns or other non-unit stride memory access behaviors for example if localmax array element scan within findlocalmax is replaced by sparse data access logic instead or conditional jumps & complex control flow driven non-linear access patterns dynamically varying per loop iteration causing scattered or non-unit stride access for cache system resulting in false sharing if threads operate within same cache lines without strict control of alignment and data partitioning precisely aligned at cache line boundaries effectively as intended in ideal programming model based assumption but in practice cache lines & physical memory alignment might not perfectly match algorithmic intent and memory allocator & OS page management mechanisms in play add further complexities leading to subtle False Sharing scenarios hard to predict without detailed runtime profiling and hardware counter instrumentation and analysis during performance debugging activities for real HPC applications – and initial code example provided here in prompt while simple enough is still susceptible to False Sharing in principle depending on array layout & alignment and HW caching behavior at runtime and threads interleaving access timing effects that might create unintended cache contention at HW level due to false sharing phenomena manifestation during runtime – even for demo program like current example as given at start for analysis purposes – subtle effects are always possible and should be considered during detailed performance characterization tasks using HW perf counter profiling measurements & detailed cache level event tracing if needed for deeper insight and root cause understanding if suspected False Sharing might be a contributing factor to performance degradation if observed in runtime experiments of example demo program on target HW platform after compiling & execution process under representative workload conditions – but not explicitly present or immediately obvious via static code inspection of simplistic provided demo source excerpt directly on its own at current stage of example code analysis and instructional problem solving process for illustration goals attainment & example coverage demonstration objective accomplishment within prompt driven Q&A framework employed here for expert system level algorithmic knowledge conveyance via structured interaction format as chosen method for demonstration purposes now underway and being illustrated in text example.
        // L3-Cache: False Sharing - threads modifying *different* array elements but elements within *same cache line* due to unfortunate layout causing shared cache line invalidation overhead – even within conceptually partitioned data block for each thread assignment to calculate local maximum array range partition block independently for parallel work distribution as conceptually designed but physical memory layout alignment might not perfectly conform exactly to logical program partitioning as programmer intention if subtle misalignment between array data allocation address & starting cache line boundary causes misalignment of cache line to partition start & end blocks in memory for parallel threads assigned regions to operate upon via iteration ranges and data processing units decomposition assigned algorithmically to each thread participating in parallel execution of array scan function calls for local maxima calculation step performed in parallel over blocks of input array in demo program as illustrated via code provided here in text and prompting user query based setup of Q&A interaction scenario as method for instructional example output generation via prompt controlled knowledge transfer and HPC expertise demonstration exercise case showcase being currently performed here within context of prompt based system – with emphasis on learning by example through Q&A dialogue based interaction.
        // L4-Edge: False Sharing is subtle and intermittent – depends on data layout, cache line size, thread scheduling - hard to debug without profiling tools specifically designed for cache event analysis. Example needs running & profiling for detailed observation and quantitative assessment for definitive false sharing identification verification – static code inspection alone is insufficient to definitively confirm false sharing or rule it out with absolute certainty unless combined with HW specific architectural detail knowledge regarding cache line sizes, memory layout schemes and OS memory allocation policies at runtime.
        // L5-HW: Cache line size (64B, etc), memory layout by compiler & allocator all contribute to False Sharing risk profile of given code in specific HW execution environment being targeted for HPC deployment in real world HPC scenarios outside simple demo example setup & analysis for learning objectives as primary focus currently for example design here being discussed and demonstrated interactively via prompt driven system output example based Q&A based instructional method currently deployed via user prompting to initiate expert level answers generation algorithmically by automated AI based intelligent prompt response engine for complex software engineering knowledge domains application in specialized areas like High Performance Computing code design analysis and best practices via automated knowledge extraction and presentation methods suitable for instructional example demonstration context as being used now in prompt answer here in textual representation as response to expert level user question about HPC software bug classes and typical examples for each category demonstrated via code excerpt based inspection and hypothetical expansion examples if needed to cover all bug categories asked about initially in prompt question prompt content.
        // CTO Rec:  Cache-aware data alignment, padding. Structure of Arrays (SoA) data layout if appropriate for data access pattern of core algorithms used in HPC code being optimized for cache efficiency and false sharing avoidance – performance critical loops especially need False Sharing profiling (perf mem --hw-cache-events=...).  Code reviews emphasizing cache-conscious design in HPC codebase style guides & best practice guideline documents & team training program as proactive steps for prevention and mitigation in design phase for future software projects intended to operate in performance sensitive HPC environments and achieve scalable parallel performance as primary design objective throughout development lifecycle of software systems using parallel computing technologies effectively.

        // False Sharing Example 2:  If globalMaxValue & thread_id are located close in memory layout (unlikely here but compiler can reorder layout)– could induce False Sharing between updates to globalMaxValue and reads of thread_id by other threads’ printf. Highly theoretical, but False Sharing can arise in unexpected ways from compiler driven layout decisions, although normally less common now than array element based sharing issue pattern – global variable proximity false sharing is less likely now than in older code with more global data scope prevalent code structuring habits – more localized variable scopes via modular code structuring best practice programming guidelines help reduce false sharing via global data exposure & shared memory visibility in contemporary HPC code designs practices typically recommended as software engineering style conventions applied for high performance and scalable parallel software in general including HPC domain coding practices too – modularity, locality of reference and minimal shared global data as principles for performance focused and robust parallel program design methodologies – and false sharing awareness in memory access patterns analysis should be integral part of HPC performance engineering lifecycle starting with architectural design choices impacting memory access patterns & cache behavior characteristics for efficient utilization of HW memory subsystems and CPU cache hierarchies which play crucial role in performance & scalability characteristics of HPC applications being developed & optimized for execution on modern multicore manycore architectures and distributed systems infrastructure in contemporary HPC settings & clusters operational deployments worldwide within large scale scientific research computing centers supporting national science mission critical computing resources via federal government funded agencies and academic institutions engaged in cutting edge research using computational modeling and simulation as key tool of science & engineering innovation driven technological advancements impacting various sectors of economy, societal wellbeing and human knowledge expansion and exploration objectives.
        // L3-Cache: Adjacent globals False Sharing – if globalMaxValue and thread_id vars located within same cache line memory address range and actively updated/read by multiple threads leading to shared cache line contention even between semantically distinct variables if memory layout creates physical proximity inside single cache line scope unintentionally – Compiler driven layout and variable packing can cause unexpected adjacency and proximity based false sharing if not explicitly controlled by programmer via data structure alignment & padding design control features application & software engineering coding standards guidelines & code review process during HPC project lifecycle for performance assurance as core software quality aspect within overall project delivery lifecycle requirements and expectations to meet or exceed performance benchmarks and scaling objectives predefined during system architecture design phase of software product development timeline process execution management milestones and success criteria for software engineering project success definition & validation procedures at quality control steps.
        // L4-Edge: Compiler optimizations, variable layout rearrangements – can unexpectedly place unrelated variables close, triggering False Sharing indirectly through memory layout optimization heuristics at compiler level backend phases processing code before binary code output artifact generation stage – hidden unexpected source of False Sharing if compiler optimizations are overly aggressive or heuristic algorithm used not always perfectly aligned with application developers original intentions or mental models of program data layout expectations if not explicitly controlled by programmers through coding styles enforcing certain data structures & layout policies & alignment control using pragma directives and compiler directives as means of explicit code control of data layout generation at machine code output step instead of relying solely on default compiler heuristics & potentially suboptimal automatic optimizations performed at compilation stages – best practices in HPC coding typically include explicit data layout and memory access control strategies enforcement as core part of HPC performance optimization expertise area and methodology guidelines compliance & active engagement by developers team throughout software development lifecycle stages.
        // L5-Compiler: Compiler memory layout optimizations heuristics details influence false sharing propensity from globals proximity – Compiler *could* theoretically introduce false sharing due to aggressive optimization based variable layout choices during backend compilation pipeline stage where machine code instructions generation, register allocation & memory layout finalization steps take place to produce optimized executable binary artifact for deployment & execution on target hardware platform environments where HPC applications will be deployed operationally within real world use cases in context of scientific computing applications requiring high performance and scalable parallel processing as primary functional requirements of software solution for intended scientific research domain specific problem solving needs & computational resource utilization effectively as primary objective drivers for HPC application software engineering projects aiming to achieve research goals in science & engineering innovation process in numerous sectors of economy, academia, government and industry in various R&D organizations utilizing HPC as foundational technology for their core mission objectives and business needs fulfillment via computationally intensive modeling & simulation workflows and big data analysis at scale within HPC data center environments operated & managed by skilled engineering and scientific staff as professional resource pool.
        // CTO Rec: Code reviews for global variable usage & potential for accidental proximity driven false sharing risks assessment especially in performance critical code regions & data processing loop kernel hotpath segments requiring highest possible efficiency and low overhead for memory accesses to achieve performance targets and scalability metrics – Performance profiling tools for False Sharing detection beyond just array element level (watch for globals also using specialized memory profiling utilities tailored for cache event trace and false sharing hotspot detection across full memory address space including global variables sections in code binary and memory segments as relevant investigation focus area using advanced hardware performance monitoring tool capabilities – if deemed necessary during in depth performance bottleneck analysis stage when false sharing is suspected to be a major factor in limiting scalability or overall application execution performance for HPC workloads of interest to be optimized for improved throughput, reduced latency and better resource utilization efficiency for HPC infrastructure hardware assets and software application systems running atop those systems via coordinated HW/SW engineering optimization efforts aimed at maximizing return on investment from HPC infrastructure and software system design for scientific productivity & technological advancement in targeted domain of scientific inquiry & engineering application using advanced computational modeling & simulation technologies in conjunction with data driven methods and AI/ML approaches being deployed at scale in modern HPC systems across various domains of science and engineering currently being explored actively in research & development centers worldwide leveraging HPC as foundational infrastructure technology for scientific and technological innovation agenda advancement goals).

        // Cache Thrashing Example 1: If N is *huge* and findLocalMax iterates with large stride (imagine modified findLocalMax with stride > 1 access in loop over very large input data arrays much larger than cache size limit) - cause Cache Thrashing due to repeatedly accessing data beyond cache capacity in each thread loop execution phase in modified hypothetical scenario via imagined code function modifications – currently loop is simple unit stride cache friendly but if stride increases & dataset > cache Thrashing manifests as performance bottleneck indication under those hypothetical modification conditions intended to illustrate example case type as per prompt instruction requirements to provide examples of each listed HPC bug classes identified and discussed within prompt as pre-requisite input and expectation.
        // L3-Cache: Large stride access pattern over very big ‘u’ array (hypothetical modification of findLocalMax for demonstration purpose) > cache size -> Cache Thrashing guaranteed via repeated cache misses from strided access & high miss ratio in cache hierarchy due to replacement of relevant cache lines too frequently leading to low cache hit rates & high memory latency penalty overall system level performance impacting bottleneck manifestation due to thrashing in cache levels leading to CPU stalls and low instruction throughput per cycle due to memory access delays dominantly controlling performance and throughput via cache system thrashing effect observation during profiling if monitored via hardware counters performance measurement & visualization in charts/graphs showing cache miss rate as high indicator of potential thrashing issue prevalence – high cache miss rate diagnostic key metric.
        // L4-Edge: Performance collapses dramatically when stride & array size reach thrashing regime in hypothetical stride-modified code example version designed for demonstration of Cache Thrashing as HPC bug class type illustrative scenario example creation objective to address prompt requirements fully by illustrating each requested bug class via code example (actual or hypothetical illustrative as appropriate for demonstrating the bug type in clear manner to demonstrate example type classification in context of prompt answer generation task to fulfill initial HPC expert questioner instructional need for understanding & knowledge gain through QA structured knowledge sharing via example illustrative cases). Performance cliff in graph for execution time vs stride/array size – sharp degradation observed if performance is plotted against varying input parameter range of values.
        // L5-HW: Cache replacement policies (LRU, etc) exacerbated thrashing for stride > 1 and large dataset sizes in simulated modification use case demonstrating thrashing – HW cache line eviction & replacement becomes bottleneck due to algorithm data access pattern mismatch with cache architecture efficiency leading to performance collapse by cache system itself failing to serve CPU with data effectively and memory latency dominating CPU cycles consumed during instruction execution within tight performance critical loops involved in core algorithm execution patterns during computation runtime if thrashing scenario is actively manifesting and limiting performance as observed outcome or measured result via empirical performance tests designed & executed as part of HPC code development and performance validation workflow process steps performed during HPC project life cycle phases and iterations stages of development and refinement based software engineering methodologies and best practices being adopted and followed by professional HPC software engineering teams building high performance and scalable parallel application solutions for scientific computing problem solving in domain specific context needs.
        // CTO Rec: Algorithmic redesign for cache locality – tiling/blocking, data reordering, prefetching to reduce cache thrashing (algorithm level & data structure redesign techniques).  Cache blocking algorithm optimization for matrix ops, stencils etc if such algorithms become primary performance hotpots in real world extended code scenarios compared to simplified example provided at prompt stage for analysis and example illustrations needed for knowledge conveyance purposes via QA demonstration approach applied. Memory access pattern optimization at algorithm level - stride 1 or very small strides is often key target design strategy for HPC performance in cache based architectures dominant now across all CPU and GPU platforms for high performance computing – achieving cache friendly code via access pattern control in algorithmic design – fundamental principle of HPC optimization – locality of reference in time and space domain being critical design factor to maximize cache hit rates and minimize cache miss penalties in performance sensitive application kernels and data processing stages – algorithmic redesign sometimes necessary when cache thrashing fundamentally limits performance due to inherent algorithm characteristics requiring scattered or non-unit stride memory access as essential operational elements causing cache system inefficiency as intrinsic algorithmic behavior in conflict with HW architecture limitations. Algorithmic modifications needed in such situations if performance impact is unacceptable & optimization is mandatory to improve throughput and reduce latency of critical code path execution durations for targeted application workflow performance needs.

        // Cache Thrashing Example 2: Thread Oversubscription in simple demo code by setting too many threads relative to array size for *N* in `num_threads(12)`  even for small N, imagine *many* threads in Thread Oversubscription condition competing for same cache lines for very small array regions causing cache thrashing – from thread context switching and excessive thread count compared to effective work decomposition units per thread in extreme oversubscription conditions manifesting – thread count >> data chunks, leading to cache thrashing symptom – albeit subtle and less dominant than large stride example but still possible effect in Thread Oversubscription case due to cache contention between excessive threads and cache coherence traffic induced overhead due to threads competing for limited cache resources under extreme Thread Oversubscription ratio compared to available data to be processed per thread work unit assignments as distributed load in parallel program sections – current example simple so oversubscription less dominant impact than large stride but theoretically relevant issue class manifestation still possible in extreme Thread Oversubcription regimes.
        // L3-Cache: Thread Oversubscription (num_threads >> N or >> core count) – too many threads context switching frequently pollute cache – Cache Thrashing indirectly induced by context switching & thread competition for limited cache resources under extreme Thread Oversubscription levels for given workload scenario complexity characteristics related to problem decomposition size per parallel unit compared to thread count assigned for parallel execution framework context deployment under consideration for HPC task performance optimization using parallel computing technology effectively and efficiently given application computational demands and architectural platform features of target HPC system chosen for deployment of software solution via implementation effort and runtime execution stages operational flow through under operational load profiles.
        // L4-Edge: Performance *degrades* as thread count increases *beyond* optimal for given problem size *N* – Thread Oversubscription leads to perf decrease as thread count rises in extreme oversubscription regime because of cache thrashing related to thread context switch overheads dominating useful computation for thread oversubscription induced performance degradation cases in practice as system wide performance symptom from algorithmic parameter setup mismatch (too many threads compared to actual parallelism available and problem data partition sizes appropriate for thread count given & effective parallelism degree possible for problem class under consideration to exploit and optimize within practical limits of scalability defined by algorithmic and system constraints for specific application cases under focus for optimization and performance improvement efforts targeting specific benchmark criteria to meet requirements as performance goals.) Performance drops unexpectedly in thread scaling experiment – initially linear scale then performance decline point indicating onset of Thread Oversubscription cache thrashing induced slowdown due to increased cache coherence traffic and context switch overhead dominating parallel speedup benefit from thread count increase past certain threshold defined by system and algorithmic properties & resource competition and scheduling dynamics in HPC environment running concurrently within multi-user operational setup typically prevalent in shared HPC cluster system scenarios – single node case in isolation less prone to Thread Oversubscription performance impact manifestation than cluster scale scenario due to less system wide contention overall usually.
        // L5-OS: OS scheduler thrashing caches by frequent context switches between excessive thread counts competing for CPU cycles & memory resources – OS level context switching overhead becomes dominant bottleneck driver for performance degradation in Thread Oversubscription regime causing Cache Thrashing symptom – OS scheduling algorithms & cache coherence protocol interactions leading to perf collapse due to thread count mismatch relative to workload & problem size and resource capacity for cache subsystems under thread oversubscription pressure causing system level cache thrashing issues and performance loss for applications deployed with too many threads and inefficient resource utilization via improper thread count configuration settings during application runtime configuration process in HPC execution environment or workload deployment system parameter setups stage of operation flow lifecycle stage prior to actual production runtime execution step commencement within cluster system workload management workflow sequences operational control mechanisms to ensure effective resource utilization & performance guarantees in HPC infrastructure under user job load profile in real world scientific computing scenarios & use cases demands typical of large scale HPC facilities management objectives as infrastructure providers role within research computing centers supporting diverse user community with varied workload characteristics needing shared access to powerful compute resources for scientific problem solving tasks via high performance software systems running in parallel execution modes to achieve targeted computational throughput & solve large scale computationally intensive problems in diverse scientific domains requiring HPC tools and technologies to advance research agenda and innovation efforts within science & engineering disciplines across various fields leveraging advanced computational methodologies via HPC software application deployment for large scale problem simulations & data analysis workflows – requiring specialized software engineering expertise and system level administration skill sets to manage & operate such complex computing platforms & application software systems effectively & efficiently to maximize scientific productivity & user satisfaction & cost efficiency for research resource investments within public funded HPC infrastructure projects worldwide aimed to benefit society and promote science for human progress via computational discoveries and innovation advancement outcomes derived from HPC research infrastructure enabling large scale science & engineering research activities effectively via resource sharing, collaboration, knowledge dissemination and software tools development support ecosystems within scientific community worldwide globally engaging in open science principles and international research cooperation via global research networks in various scientific domains contributing to advancement of human knowledge & technological progress in various application areas benefitting from HPC technology applications via multidisciplinary research & development collaborations & large scale infrastructure projects in scientific computing fields using HPC as essential foundational technological capability for problem solving needs requiring high performance and scalable computational resources beyond desktop computing limits for next generation science discovery driven by data and compute intensive approaches to address complex real world scientific & engineering challenges via HPC empowered software systems engineering methods and technologies as applied domain expertises driving innovation forward via computationally enabled science and engineering progress via high performance computing tools usage and research outcomes worldwide through scientific research enabled by HPC infrastructure & expert services offered to researchers by large scale computing centers & organizations across the globe collaborating and contributing to global scientific community advancement in diverse areas leveraging HPC technology for scientific discoveries and engineering innovation within global science ecosystem contributing to human knowledge base & technological progress in various societal domains via collaborative scientific efforts and research data sharing open science paradigms enabling accelerated scientific progress across the globe benefiting all of humanity via scientific breakthroughs driven by HPC enabled research & development within global science & engineering community using HPC tools for advancing scientific frontiers and solving complex societal challenges via technological innovation based on computational science & engineering methodologies enabled by HPC infrastructures and skilled expert teams providing access to these resources and software tools support for research user communities around the world seeking to address grand challenge scientific problems using computational methods & data intensive techniques via HPC facilities available via public and private investments in research computing infrastructure at scale.
        // CTO Rec: Control thread count - limit to core count or slightly above. Thread pool management for HPC apps.  Monitor context switching rates – perf sched or vmstat.  Avoid excessive Thread Oversubscription by workload distribution strategy analysis and thread count tuning to match available parallel work units and CPU resources available effectively without causing excessive overhead due to context switching or cache contention or other Thread Oversubscription related performance bottleneck inducing side effects – algorithm and workload characteristics need careful consideration when deciding optimal thread counts for parallel program execution configurations in HPC environments - optimal thread count is NOT always “more is better” beyond certain limits defined by application, system & workload parameters interplay defining system performance and scalability properties. Thread count tuning for each application use case & input dataset type & HW platform configuration often necessary step in HPC performance optimization process workflows as routine activity within application deployment lifecycle stages and for ongoing performance monitoring & maintenance activities as routine system performance management operations & capacity planning for future HPC infrastructure investments based on usage patterns and application needs projection for anticipated research demand over timeframes defined via scientific roadmap strategic planning processes for research infrastructure projects aimed at enabling long term research goals for scientific advancement using HPC technologies & expert workforce as crucial components of research infrastructure facilities & operational services for research community to utilize effectively for advancing knowledge and technology across multiple domains and scientific fields


        int thread_id = omp_get_thread_num();
        int total_threads = omp_get_num_threads();
        double localMaxValue;

        int base_block_size = N / total_threads;
        int remainder = N % total_threads;
        int block_size = base_block_size + (thread_id < remainder ? 1 : 0);
        int start_index;

        if (thread_id < remainder) {
            start_index = thread_id * (base_block_size + 1);
        } else {
            start_index = remainder * (base_block_size + 1) + (thread_id - remainder) * base_block_size;
        }
        int end_index = start_index + block_size;

        // Initialize the array section
        for (int i = start_index; i < end_index; ++i) {
            u[i] = (double)i;
        }

        localMaxValue = findLocalMax(start_index, end_index, u);

        #pragma omp critical
        {
            // --- CPU/GPU Performance Pitfalls ---
            // Vectorization Failure Example 1: Compiler fails to vectorize simple loop due to loop-carried dependency (even when dependency is actually removable by code change).
            // L2-Algo: Simple sum loop - compiler SHOULD vectorize.  BUT add seemingly small dependency (e.g., indirect array access with potential overlap) and vectorization may FAIL by compiler conservatism even if dependency actually NOT performance critical. Vectorization Failure -> scalar loop = HUGE perf drop vs SIMD.
            // L4-Edge: Subtle code change - seemingly minor - causes vectorization to silently *disable* due to overly conservative compiler heuristics = Vectorization Failure without warning -> silent performance regression hard to detect in code review alone unless you *know* to check assembly code or compiler vectorization reports!
            // L5-Compiler: Compiler vectorization heuristics (dependency analysis) imperfect. Can fail to vectorize even vectorizable loops under certain conditions, Compiler imperfection => Vectorization Failure bugs often silent and need assembly analysis or compiler report to even *know* they happened, not obvious in source code inspection easily in complex loops.
            // CTO Rec: Compiler optimization reporting ENABLED by default (vectorization reports), Assembly code inspection for key HPC loops MANDATORY, vectorization pragmas for compiler hints IF necessary after careful analysis but pragmas can ALSO HINDER in some cases if used wrongly.

            // Vectorization Failure Example 2: Data type or function call inside loop PREVENTS vectorization - e.g., double precision division or non-vectorizable math function call INSIDE loop body -> Vectorization Failure.
            // L2-Algo: Double division, some complex math function calls (sin, cos, etc) in loop BODY - compiler may give up vectorizing loop completely -> Vectorization Failure leading to scalar loop again HUGE perf drop if vectorized version was expected!
            // L3-CPU: SIMD units operate BEST on floats/ints/SIMD friendly ops. Double div, complex math, can limit vectorization opportunity by compiler.
            // L4-Edge: Small change – adding double precision or complex math function call in loop -> VECTORIZATION FAILURE – major performance SLOWDOWN silently. Hard to see without perf tools or assembly review!
            // CTO Rec: Compiler flags optimization level CHECK, Target ISA verify (AVX512 for doubles helps), code review for double division or complex math INSIDE innermost HPC loops. Consider fast approximation methods, float precision, if double division etc is bottleneck and if vectorization essential for inner loop performance. Compiler reports, assembly inspect to diagnose vectorization fail cases related to double precision ops inside vector loops especially double division operations causing vectorization aborts.

            // Thread Oversubscription Example 1: Creating TOO MANY threads relative to available cores, leading to context switch overhead exceeding parallel speedup - imagine HPC code creates threads per DATA element instead of per CORE – Thread Oversubscription imminent.
            // L3-OS: Creating threads PER DATA element, NOT core count aware. System gets FLOODED with threads >> core count = Thread Oversubscription perf implosion.
            // L4-Edge: Performance *degrades* with increasing thread count beyond core count due to Thread Oversubscription context switching storm, counterintuitive to parallel speedup initially expected from simply "more threads == faster" - NO.
            // L5-OS: OS scheduler thrashing to context switch among excessive threads. Kernel scheduling overhead DOMINATES useful work from app - Thread Oversubscription = Perf Cliff.
            // CTO Rec: Control thread count.  Core count detection. Thread pool management to limit thread count to *reasonable multiple* of core count – not unbounded thread creation per data item that causes Thread Oversubscription perf collapse.  Performance monitoring tools to detect context switching rates to diagnose Thread Oversubscription overhead (perf sched, vmstat tools OS level thread monitoring).

            // Thread Oversubscription Example 2: Nested parallelism WITHOUT thread pool management causes EXPONENTIAL thread explosion leading to EXTREME Thread Oversubscription - OpenMP collapse without thread limit -> Thread Oversubscription doom.
            // L3-OS: NESTED PARALLELISM *without* thread pool LIMITS – Inner and outer parallel loops each create FULL thread teams – Exponential thread count = Thread Oversubscription CATASTROPHE, system MELTDOWN of context switching overload.
            // L4-Edge: Nested OpenMP, especially nested parallel loops without collapse clause, easily trigger extreme thread count explosion, silent Thread Oversubscription leading to perf PLUMMET suddenly in nested parallel patterns WITHOUT careful thread management & thread limiters.
            // L5-OS: Kernel scheduler CRUSHED under EXPLOSION of threads from nested parallelism Thread Oversubscription. System becomes unresponsive due to extreme thread mgmt overhead of Kernel, amplified by context switching cost as thread count grows exponentially with nesting depth without control mechanisms enforced.
            // CTO Rec: Thread pool management CRITICAL for HPC - RESTRICT max thread count. Avoid uncontrolled Nested Parallelism thread explosion.  OpenMP collapse clause or explicit thread number control.  Profile context switching rates, thread counts during nested parallel regions, mandatory resource limit enforcement to avoid runaway thread creation.

            // Load Imbalance Example 1: Uneven loop iterations workload distribution among threads in #pragma omp parallel for causes Load Imbalance -> some threads Starve for work. Assume conditional BREAK inside loop iterations, making iteration times very unequal -> Load Imbalance becomes clear performance bottleneck.
            // L2-Algo: Add conditional break INSIDE loop. Simulates uneven loop iteration cost - Load Imbalance is NOW manifest due to varying work per iteration in loop if workload per iter non-uniform causing Load Imbalance performance hit from barrier.
            // L3-HPC: #pragma omp parallel for + barrier - loop iteration time VARIANCE -> Load Imbalance visible at barrier synchronization, slower threads bottleneck overall parallel speed. Faster threads STARVE waiting at barrier for slow threads, leading to performance wastage in loop.
            // L4-Edge: Data dependent conditional code inside loop – load imbalance SEVERELY worsened if condition frequency varies SIGNIFICANTLY across data subsets leading to skewed thread workloads and subsequent Load Imbalance and Starvation effects at implicit OpenMP loop barrier sync point after loop finishes iteration phases.
            // CTO Rec: Dynamic scheduling in #pragma omp parallel for clause (dynamic, guided clauses) to mitigate Load Imbalance when iteration times uneven or data dependent.  Workload profiling, per thread iteration time tracing to QUANTIFY Load Imbalance degree for specific loop, tune OpenMP scheduling for optimal Load Balancing for given workload pattern observed empirically.

            // Load Imbalance Example 2:  Tasks in OpenMP sections with vastly different execution times -> Load Imbalance Starvation at implicit barrier after sections - sections not balanced. Imagine task_a, task_b, task_c different complexities drastically.
            // L2-Algo: task_a, task_b, task_c replace trivial printfs with functions of VERY DIFFERENT complexities – Imbalance manifest. Some sections take MUCH longer than others = Load Imbalance between sections execution units in parallel section construct.
            // L3-HPC: OpenMP #pragma omp sections implicit barrier at end - section execution time variance causes Load Imbalance - faster sections complete, then threads STARVE waiting for slow sections to finish at barrier.  System idle while waiting Load Imbalance at section barrier sync.
            // L4-Edge: Load Imbalance between sections SEVERELY degrades parallel efficiency as slower sections dictate overall runtime, and faster sections remain idle after completion before barrier. Load Imbalance across parallel section blocks leads to performance bottlenecks even if individual section code itself is highly optimized locally in each section code, section decomposition imbalance becomes issue to fix then via better work balancing between sections decomposed task units or algorithm task unit granularity adjustment.
            // CTO Rec: Work decomposition analysis for parallel sections is critical to LOAD BALANCE section execution times. Tasks assigned to parallel sections MUST have approximately equal computational cost and duration for efficient parallel sections usage pattern, and Load Imbalance from uneven sections workload MUST be minimized via algorithm redesign, work unit redistribution across section, or adjusting granularity to enforce Load Balancing. Task-based OpenMP parallelism (vs sections based purely) MIGHT offer finer grained dynamic scheduling to help in dynamic Load Imbalance mitigation IF tasks naturally decompose into smaller independently scheduled work units with better runtime load distribution.  Profiling tools for section execution time analysis for LOAD IMBALANCE diagnostic detection - instrumentation tools for per-section runtime metrics measurement.

            // High Register Pressure Example 1: Aggressive loop unrolling by compiler + very complex loop body -> register spilling and perf degradation from register pressure induced spilling. Loop unrolling gone TOO FAR!
            // L2-Algo: Loop unrolling is compiler opt, good initially. BUT *excessive* unrolling, especially in complex loop bodies -> register pressure. High Register Pressure triggers Register Spilling.
            // L3-CPU: CPU register file limited. High Register Pressure => Compiler forced Register Spilling - register data forced to SLOWER MEMORY - performance cliff.
            // L4-Edge: Subtle compiler optimization (loop unrolling aggressiveness) BACKFIRES badly in perf due to register pressure triggering register spilling overhead dominating speedup. Auto-optimization FAILURE case where auto compiler decisions negatively impact performance if not CAREFULLY monitored.
            // L5-Compiler: Compiler register allocation algorithms pushed to limit by excessive loop unrolling, forced register spills in tight loops causing major performance penalty if Register Pressure exceeds HW register file capacity limits during instruction scheduling phases in compiler code generation step. Compiler optimization tradeoff misfire scenario from overly aggressive loop unrolling decision under register pressure.
            // CTO Rec: Compiler flags for loop unrolling CONTROL, experiment - less aggressive unroll maybe better. Assembly code inspect for register spills in key loops (compiler output). Profile with/without unrolling - verify unroll IS helping not HURTING perf by triggering register pressure spill impact and overall instruction cycle count increased despite initial intentions of loop unrolling opt aimed for improved code execution path cycles minimization instead potentially and paradoxically causing slowdown in practice on certain microarchitectures with limited register files relative to loop unroll levels generated automatically.

            // High Register Pressure Example 2: Very deeply nested function calls within innermost loops - Deep call stacks accumulate register pressure on call frames, causing Register Spilling within loops even without explicit loop unrolling itself - Stack frame size related register pressure causing Spilling not loop unroll directly this time.
            // L2-Algo: Innermost loops - keep bodies SIMPLE!  Deeply nested functions inside loops cause STACK FRAME register pressure build up -> Register Spilling perf doom. Avoid deep call nests INSIDE innermost hot loops in HPC for register pressure control strategy enforcement needed in HPC coding best practice coding guideline context application.
            // L3-CPU: Function call ABI register conventions push arguments/return address onto stack. Deep nesting CALL STACK buildup rapidly EXHAUSTS registers leading to register spills within loop itself *due to stack frame register pressure buildup* - call stack depth driven Register Pressure cause.
            // L4-Edge: Performance collapse NOT obvious in source - subtle stack frame driven Register Pressure causing spills hard to see from high level C++ source directly - requires deeper assembly or perf counter based register pressure diagnostic and analysis via tooling and profilers and detailed microarchitectural low level tracing/inspection.
            // L5-Compiler: Compiler stack frame management efficiency & register allocation around function calls IN INNER LOOPS is key for Register Pressure control here if loop body complexity is call dominated NOT instruction level complex linear sequence necessarily to analyze Register Pressure source – call nesting is now key culprit driver behind register pressure spills. Compiler inter-procedural optimization may or may not mitigate deep call nesting driven Register Pressure automatically - profile & verify, and maybe inline function calls manually if critical path loop to reduce call frame depth effect & improve register allocation in hot inner loops via call nesting reduction.
            // CTO Rec: Minimize function call depth within performance-critical innermost loops.  Inlining strategic function calls.  Assembly code inspection register allocation in inner loops. Function call stack depth LIMITATION/guidelines coding style enforcement within perf hotspots of HPC codes via design principles of HPC modular code with flat loop nesting instead of deep function calls especially INSIDE inner performance critical hotspot loops.

            // Register Spilling Example 1: Directly caused by High Register Pressure above, Spilling is CONSEQUENCE. Spills themselves degrade performance. Show code that induces High Register Pressure to SHOW Register Spilling symptom manifesting then. Spilling is NOT bug in code necessarily itself - but RESULT of High Register Pressure code induced scenarios listed before, consequence of HW resource limits hit & SW coding patterns combined. Register Spilling by ITSELF is not "bug" - more a "performance DEGRADATION SYMPTOM" - triggered by UNDERLYING CAUSE, High Register Pressure or other factors inducing HW resource limits exceedance of register files in CPUs. Register Spilling is RESULT - not ROOT cause of a BUG but symptom of underlying issues, e.g. high register pressure code induced cause listed above and before to focus on instead of “Register Spilling bug” as much because spilling is consequence. Focus prompts on *causes* of register spills as bugs and perf pitfalls: High Register Pressure or similar causes instead of “register spilling as bug” focus alone by itself unless you mean “Register Spilling as *performance PROBLEM indicator*" NOT as bug type on its own exactly.  Focus prompt on CAUSES not consequences as much.

            // Register Spilling Example 2: Loop with extremely LARGE number of local variables ALL live at once - ALL requiring registers SIMULTANEOUSLY - -> High Register Pressure, Forced Register Spilling. Loop with just TOO MANY variables all alive at once even in flat code WITHOUT function calls - variable count alone pushes register pressure.
            // L2-Algo: Loop with EXTREME count of LOCAL vars all "live" at loop iterations causes huge Register Pressure, Spilling even without loop unroll/function calls. Variable COUNT complexity issue this time - not code execution path instructions now mainly driving Register Spilling, but just TOO MANY simultaneous live registers required by simple loop variable declarations themselves.
            // L3-CPU: Limited register file - TOO MANY "live" variables at once within loop code block – registers EXHAUSTED = Register Spilling. Loop body code could be simple instructions themselves but VAR count overload now pushes register file resource beyond capacity limits – register spilling as CONSEQUENCE.
            // L4-Edge: Refactor loop - reduce simultaneous "live" variable count by REUSING variables or reducing scope - to mitigate Register Spilling. Profile register spills - confirm variables count directly contributing to spilling via profiling and assembly analysis and perf counters, verify and then reduce live vars.
            // L5-Compiler: Compiler register allocation – forced to spill excess loop variables to memory because loop body has just TOO MANY locals for available HW registers on CPU core, compiler’s register allocation algo is just limited by HW register count constraints when faced with too many concurrent live local variable definitions and compiler registers tracking requirements enforced in given scope of innermost performance hot loop section – hence spills caused due to live local variable count explosion driven pressure imposed by loop code structure complexity driving Register Spilling consequence instead as root bug now here of too many live loop local vars. Redesign loop for lower local variable count requirements, reuse variable space instead of new decl for every purpose if functionally equivalent logically within code to reduce simultaneous register demand and spilling issues caused by live variable pressure limits imposed by loop coding patterns creating too many live local variable simultaneous requirement within loop code block register alloc scopes at compiler code generation stage phase operations performed by compiler itself to produce optimized compiled loop machine code performance profile after optimization via lower register pressure design coding principle and approach implementation enforcement throughout inner performance critical hot loop codes areas needing low register footprint to improve instruction level performance by fitting register usage efficiently within limited register file HW resource constraint capacities via loop code structure redesign aimed at minimizing live simultaneous variable count within hot loops to achieve lower register pressure for spill prevention benefit maximizing performance.

            if (localMaxValue > globalMaxValue) {
                globalMaxValue = localMaxValue;
            }
        }
        printf("Thread %d: Local Max Value: %lf (Indices: %d to %d)\n", thread_id, localMaxValue, start_index, end_index - 1);
    }

    printf("Global Maximum Value: %lf\n", globalMaxValue);
    free(u);
    return 0;
}
```
