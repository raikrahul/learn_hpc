---
layout: default
title: "Lesson 6"
date: 2024-03-09
---

<h1>Lesson 6</h1>

<p>This lesson delves into optimizing parallel code, focusing on minimizing cache contention and handling large-scale parallel computations.</p>

<h2>Cache Line Ping-Pong</h2>

<p>Cache line ping-pong arises when multiple cores repeatedly write to different memory locations <em>within the same cache line</em>. This leads to excessive cache invalidation and data transfer between cores, severely impacting performance. Modern CPUs use caches (small, fast memory) to speed up data access. Data is transferred between main memory and the cache in blocks called cache lines. When multiple cores write to different parts of the same cache line, each write invalidates the cache line in the other cores, causing a lot of back-and-forth ("ping-pong") of the cache line.</p>

<h3>Example 1: Demonstrating Ping-Pong</h3>

<pre><code class="c">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;omp.h&gt;

#define ARRAY_SIZE 1024 * 1024  // Larger array for more pronounced effect
#define NUM_THREADS 4

int main() {
    int* shared_array = (int*)malloc(ARRAY_SIZE * sizeof(int));

    for (int i = 0; i &lt; ARRAY_SIZE; i++) {
        shared_array[i] = 0;
    }

    double start_time = omp_get_wtime();

#pragma omp parallel num_threads(NUM_THREADS)
    {
        int thread_id = omp_get_thread_num();
        int chunk_size = ARRAY_SIZE / NUM_THREADS;
        int start = thread_id * chunk_size;
        int end = (thread_id + 1) * chunk_size;

        for (int i = start; i &lt; end; i += 64) { // Stride to increase cache line contention
            shared_array[i] = thread_id;
        }
    }

    double end_time = omp_get_wtime();

    printf("Time taken (with ping-pong): %f seconds\n", end_time - start_time);

    free(shared_array);
    return 0;
}
</code></pre>

<p>This example creates a shared array and has multiple threads write to it. The <code>i += 64</code> stride is crucial. It increases the likelihood that different threads will be writing to locations within the same cache line, thus demonstrating the ping-pong effect.</p>

<h3>Example 2: Mitigating with Padding</h3>

<pre><code class="c">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;omp.h&gt;

#define ARRAY_SIZE 1024 * 1024
#define NUM_THREADS 4
#define CACHE_LINE_SIZE 64 // Adjust based on your system (use lscpu on Linux)

typedef struct {
    int data;
    char padding[CACHE_LINE_SIZE - sizeof(int)];
} padded_int;

int main() {
    padded_int* shared_array = (padded_int*)malloc(ARRAY_SIZE * sizeof(padded_int));

    for (int i = 0; i &lt; ARRAY_SIZE; i++) {
        shared_array[i].data = 0;
    }

    double start_time = omp_get_wtime();

#pragma omp parallel num_threads(NUM_THREADS)
    {
        int thread_id = omp_get_thread_num();
        int chunk_size = ARRAY_SIZE / NUM_THREADS;
        int start = thread_id * chunk_size;
        int end = (thread_id + 1) * chunk_size;

        for (int i = start; i &lt; end; i++) {
            shared_array[i].data = thread_id;
        }
    }

    double end_time = omp_get_wtime();

    printf("Time taken (with padding): %f seconds\n", end_time - start_time);

    free(shared_array);
    return 0;
}
</code></pre>

<p>Padding is a technique to ensure that data accessed by different threads resides in different cache lines. The <code>padded_int</code> struct adds extra bytes to each integer, filling up the cache line. This reduces the chance of false sharing.</p>

<h3>Example 3: Mitigating with Robust Padding and Alignment</h3>

<pre><code class="c">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;omp.h&gt;

#define ARRAY_SIZE 1024 * 1024
#define NUM_THREADS 4
#define CACHE_LINE_SIZE 64

typedef struct {
    int data;
    char padding[CACHE_LINE_SIZE - sizeof(int)];
} padded_int;

int main() {
    // Allocate with extra padding to ensure chunk alignment
    padded_int* shared_array = (padded_int*)aligned_alloc(CACHE_LINE_SIZE, ARRAY_SIZE * sizeof(padded_int) + (NUM_THREADS - 1) * CACHE_LINE_SIZE);
    if (shared_array == NULL) {
        perror("aligned_alloc failed");
        exit(1);
    }

    for (int i = 0; i &lt; ARRAY_SIZE; i++) {
        shared_array[i].data = 0;
    }

    double start_time = omp_get_wtime();

#pragma omp parallel num_threads(NUM_THREADS)
    {
        int thread_id = omp_get_thread_num();
        int chunk_size = ARRAY_SIZE / NUM_THREADS;
        // Calculate the aligned start address for each thread.
        padded_int* thread_start = shared_array + thread_id * chunk_size;

        for (int i = 0; i &lt; chunk_size; i++) {
            thread_start[i].data = thread_id;
        }
    }

    double end_time = omp_get_wtime();

    printf("Time taken (with robust padding): %f seconds\n", end_time - start_time);

    free(shared_array);
    return 0;
}
</code></pre>

<p>This example builds on the previous one. It uses <code>aligned_alloc</code> to allocate memory that is aligned to the cache line size. This ensures that the <em>start</em> of each thread's chunk of the array is also on a separate cache line. This is crucial for truly mitigating false sharing. It also checks the return value of <code>aligned_alloc</code>.</p>

<h2>Intermittent Sum Corruption at Scale</h2>

<p>With a massive number of threads (e.g., &gt; 1 million), subtle race conditions or limitations in the OpenMP runtime's reduction implementation can lead to intermittent errors in calculations. This is especially true for reductions, where partial results from many threads need to be combined.</p>

<h3>Example 4: Demonstrating Potential Corruption at Scale</h3>

<pre><code class="c">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;omp.h&gt;

#define ARRAY_SIZE 100000000 // Large array
#define NUM_THREADS 1000000  // Massive number of threads

int main() {
    long long* arr = (long long*)malloc(ARRAY_SIZE * sizeof(long long));
    if (arr == NULL) {
        perror("Memory allocation failed");
        exit(1);
    }
    long long global_sum = 0;

    for (int i = 0; i &lt; ARRAY_SIZE; i++) {
        arr[i] = i + 1;
    }

#pragma omp parallel num_threads(NUM_THREADS) reduction(+:global_sum)
    {
        long long local_sum = 0;
#pragma omp for
        for (int i = 0; i &lt; ARRAY_SIZE; i++) {
            local_sum += arr[i];
        }
        global_sum += local_sum;
    }

    long long expected_sum = (long long)ARRAY_SIZE * (ARRAY_SIZE + 1) / 2;
    printf("Calculated Sum: %lld\n", global_sum);
    printf("Expected Sum: %lld\n", expected_sum);

    free(arr);
    return 0;
}
</code></pre>

<p>This code calculates the sum of a large array using a huge number of threads.
